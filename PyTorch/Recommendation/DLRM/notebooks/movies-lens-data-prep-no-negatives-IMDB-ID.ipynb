{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "unzip is already the newest version (6.0-21ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt install unzip\n",
    "mkdir -p data\n",
    "cd data\n",
    "if [ ! -f \"ml-25m.zip\" ]; then\n",
    "    echo \"Downloading data\"\n",
    "    wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "    unzip ml-25m.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-20m\tml-20m.zip  ml-25m  ml-25m.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RATINGS = 20\n",
    "USER_COLUMN = 'userId'\n",
    "ITEM_COLUMN = 'movieId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using IMDB ID\n",
    "df = pd.read_csv('./data/ml-25m/ratings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('./data/ml-25m/links.csv', dtype={'imdbId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(links, how = 'left', on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0110912</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "      <td>0110912</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0111495</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "      <td>0111495</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0108394</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "      <td>0108394</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0114787</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "      <td>0114787</td>\n",
       "      <td>11902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0045152</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "      <td>0045152</td>\n",
       "      <td>872.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp   imdbId   tmdbId\n",
       "0       1  0110912     5.0  1147880044  0110912    680.0\n",
       "1       1  0111495     3.5  1147868817  0111495    110.0\n",
       "2       1  0108394     5.0  1147868828  0108394    108.0\n",
       "3       1  0114787     5.0  1147878820  0114787  11902.0\n",
       "4       1  0045152     3.5  1147868510  0045152    872.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['movieId'] = df['imdbId']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with less than 20 ratings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Filtering out users with less than {} ratings\".format(MIN_RATINGS))\n",
    "grouped = df.groupby(USER_COLUMN)\n",
    "df = grouped.filter(lambda x: len(x) >= MIN_RATINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to sort before popping to get last item\n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "# clean up data\n",
    "del df['rating'], df['timestamp']\n",
    "df = df.drop_duplicates() # assuming it keeps order\n",
    "\n",
    "# now we have filtered and sorted by time data, we can split test data out\n",
    "grouped_sorted = df.groupby(USER_COLUMN, group_keys=False)\n",
    "test_data = grouped_sorted.tail(1).sort_values(by=USER_COLUMN)\n",
    "# need to pop for each group\n",
    "train_data = grouped_sorted.apply(lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target']=1\n",
    "test_data['target']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = np.unique(train_data['userId'])\n",
    "items = np.unique(train_data['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0167261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0099088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0096874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0119177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0050212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  target\n",
       "36       1  0167261       1\n",
       "13       1  0099088       1\n",
       "12       1  0096874       1\n",
       "11       1  0119177       1\n",
       "9        1  0050212       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[['userId','movieId', 'target']]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0338013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "      <td>0080801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>3</td>\n",
       "      <td>0118715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>4</td>\n",
       "      <td>0071853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>5</td>\n",
       "      <td>0053271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  target\n",
       "48         1  0338013       1\n",
       "148        2  0080801       1\n",
       "307        3  0118715       1\n",
       "919        4  0071853       1\n",
       "1226       5  0053271       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[['userId','movieId', 'target']]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DLRM_data(data, filename='dlrm_data.tsv'):\n",
    "    print(\"Writing %d samples\"%data.shape[0], filename)\n",
    "    with open(filename, 'wt') as f:\n",
    "        for i in tqdm(range(data.shape[0])):\n",
    "            f.write('%d\\t%d\\t%d\\t%s\\n'%(data[i,2], 1, data[i,0], data[i,1])) #label, dummy numeric feat., userID, itemID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np= train_data.values\n",
    "np.random.shuffle(train_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np= test_data.values\n",
    "test_data_np_neg = np.zeros_like(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np_neg[:,0] = np.random.randint(1,len(users),test_data_np_neg.shape[0])\n",
    "test_data_np_neg[:,1] = np.random.randint(2,len(items),test_data_np_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[130896, 46173, 0],\n",
       "       [106913, 53325, 0],\n",
       "       [161987, 38842, 0],\n",
       "       ...,\n",
       "       [28231, 38531, 0],\n",
       "       [87930, 39152, 0],\n",
       "       [34217, 56868, 0]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_np_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np = np.concatenate((test_data_np, test_data_np_neg), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, '0338013', 1],\n",
       "       [2, '0080801', 1],\n",
       "       [3, '0118715', 1],\n",
       "       ...,\n",
       "       [28231, 38531, 0],\n",
       "       [87930, 39152, 0],\n",
       "       [34217, 56868, 0]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30425/1079894 [00:00<00:03, 304245.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:02<00:00, 514984.05it/s]\n",
      "  5%|▍         | 51572/1079894 [00:00<00:01, 515716.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 545000.69it/s]\n",
      " 10%|▉         | 104804/1079894 [00:00<00:01, 521081.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 569235.82it/s]\n",
      "  9%|▉         | 101472/1079894 [00:00<00:01, 506162.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 558184.29it/s]\n",
      "  5%|▍         | 49493/1079894 [00:00<00:02, 494929.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 565891.18it/s]\n",
      "  9%|▉         | 95087/1079894 [00:00<00:02, 455658.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 563203.79it/s]\n",
      "  5%|▍         | 50193/1079894 [00:00<00:02, 501923.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 567715.82it/s]\n",
      "  9%|▊         | 92577/1079894 [00:00<00:02, 476814.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 556783.78it/s]\n",
      "  9%|▉         | 99848/1079894 [00:00<00:01, 496852.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 556339.80it/s]\n",
      " 10%|▉         | 102780/1079894 [00:00<00:01, 514750.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 573466.66it/s]\n",
      "  9%|▉         | 97369/1079894 [00:00<00:02, 488321.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 549007.85it/s]\n",
      "  4%|▍         | 45790/1079894 [00:00<00:02, 457898.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 545596.91it/s]\n",
      "  4%|▍         | 42002/1079894 [00:00<00:02, 420012.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 573398.20it/s]\n",
      "  5%|▍         | 51251/1079894 [00:00<00:02, 512501.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 541691.42it/s]\n",
      "  9%|▉         | 102259/1079894 [00:00<00:01, 501594.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079894 samples /data/dlrm/criteo/day_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079894/1079894 [00:01<00:00, 576798.58it/s]\n",
      "  9%|▉         | 98572/1079893 [00:00<00:02, 474007.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 566253.93it/s]\n",
      "  5%|▍         | 50488/1079893 [00:00<00:02, 504878.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 552395.50it/s]\n",
      "  9%|▉         | 100602/1079893 [00:00<00:02, 479842.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 552608.54it/s]\n",
      " 10%|▉         | 104194/1079893 [00:00<00:01, 519607.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 570774.82it/s]\n",
      " 10%|▉         | 104238/1079893 [00:00<00:01, 521822.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 570095.92it/s]\n",
      "  4%|▍         | 47340/1079893 [00:00<00:02, 473391.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 553342.24it/s]\n",
      " 10%|▉         | 105260/1079893 [00:00<00:01, 516459.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 576518.70it/s]\n",
      "  4%|▍         | 43970/1079893 [00:00<00:02, 439697.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1079893 samples /data/dlrm/criteo/day_22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079893/1079893 [00:01<00:00, 560094.83it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /data/dlrm/\n",
    "!mkdir -p /data/dlrm/criteo\n",
    "for i, data_arr in enumerate(np.array_split(train_data_np,23)):\n",
    "    write_DLRM_data(data_arr, filename='/data/dlrm/criteo/day_%d'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t18544\t0298203\r\n",
      "1\t1\t125843\t0080684\r\n",
      "1\t1\t95203\t0112573\r\n",
      "1\t1\t73639\t0258463\r\n",
      "1\t1\t132651\t0109781\r\n",
      "1\t1\t29464\t0083658\r\n",
      "1\t1\t66219\t0077405\r\n",
      "1\t1\t44389\t0134847\r\n",
      "1\t1\t152508\t0088794\r\n",
      "1\t1\t9439\t0267804\r\n"
     ]
    }
   ],
   "source": [
    "!tail /data/dlrm/criteo/day_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/325082 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 325082 samples /data/dlrm/criteo/day_23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325082/325082 [00:00<00:00, 557972.00it/s]\n"
     ]
    }
   ],
   "source": [
    "write_DLRM_data(test_data_np, filename='/data/dlrm/criteo/day_23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t94294\t0053291\r\n",
      "1\t1\t157552\t1478338\r\n",
      "1\t1\t53227\t0190524\r\n",
      "1\t1\t162488\t0086190\r\n",
      "1\t1\t53193\t0120762\r\n",
      "1\t1\t51552\t0352248\r\n",
      "1\t1\t132227\t2140479\r\n",
      "1\t1\t61050\t0095776\r\n",
      "1\t1\t119495\t0046250\r\n",
      "1\t1\t103464\t0361748\r\n"
     ]
    }
   ],
   "source": [
    "!head /data/dlrm/criteo/day_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t26437\t22077\r\n",
      "0\t1\t39502\t45843\r\n",
      "0\t1\t57945\t47617\r\n",
      "0\t1\t49756\t39439\r\n",
      "0\t1\t93145\t47952\r\n",
      "0\t1\t101697\t22366\r\n",
      "0\t1\t53529\t55809\r\n",
      "0\t1\t28231\t38531\r\n",
      "0\t1\t87930\t39152\r\n",
      "0\t1\t34217\t56868\r\n"
     ]
    }
   ],
   "source": [
    "!tail /data/dlrm/criteo/day_23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  psmisc\n",
      "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
      "Need to get 52.5 kB of archives.\n",
      "After this operation, 266 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 psmisc amd64 23.1-1ubuntu0.1 [52.5 kB]\n",
      "Fetched 52.5 kB in 2s (33.8 kB/s)\n",
      "Selecting previously unselected package psmisc.\r\n",
      "(Reading database ... \r",
      "(Reading database ... 5%\r",
      "(Reading database ... 10%\r",
      "(Reading database ... 15%\r",
      "(Reading database ... 20%\r",
      "(Reading database ... 25%\r",
      "(Reading database ... 30%\r",
      "(Reading database ... 35%\r",
      "(Reading database ... 40%\r",
      "(Reading database ... 45%\r",
      "(Reading database ... 50%\r",
      "(Reading database ... 55%\r",
      "(Reading database ... 60%\r",
      "(Reading database ... 65%\r",
      "(Reading database ... 70%\r",
      "(Reading database ... 75%\r",
      "(Reading database ... 80%\r",
      "(Reading database ... 85%\r",
      "(Reading database ... 90%\r",
      "(Reading database ... 95%\r",
      "(Reading database ... 100%\r",
      "(Reading database ... 49660 files and directories currently installed.)\r\n",
      "Preparing to unpack .../psmisc_23.1-1ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking psmisc (23.1-1ubuntu0.1) ...\r\n",
      "Setting up psmisc (23.1-1ubuntu0.1) ...\r\n",
      "total 720K\n",
      "4.0K -rwxrwxrwx 1 1000 1000 1.1K Sep  1 04:35 verify_criteo_downloaded.sh\n",
      "104K -rwxrwxrwx 1 1000 1000 103K Sep  1 04:35 submit_validation_log.txt\n",
      "256K -rwxrwxrwx 1 1000 1000 256K Sep  1 04:35 submit_train_log.txt\n",
      "104K -rwxrwxrwx 1 1000 1000 103K Sep  1 04:35 submit_test_log.txt\n",
      "196K -rwxrwxrwx 1 1000 1000 196K Sep  1 04:35 submit_dict_log.txt\n",
      "8.0K -rwxrwxrwx 1 1000 1000 4.6K Sep  1 04:35 split_dataset.py\n",
      " 20K -rwxrwxrwx 1 1000 1000  20K Sep  1 04:35 spark_data_utils.py\n",
      "8.0K -rwxrwxrwx 1 1000 1000 5.9K Sep  1 04:35 run_spark.sh\n",
      "4.0K -rwxrwxrwx 1 1000 1000 2.4K Sep  1 04:35 prepare_dataset.sh\n",
      "4.0K -rwxrwxrwx 1 1000 1000 3.2K Sep  1 04:35 parquet_to_binary.py\n",
      "   0 -rwxrwxrwx 1 1000 1000    0 Sep  1 04:35 data_prep_model.pickle\n",
      "4.0K drwxrwxrwx 2 1000 1000 4.0K Sep  1 04:35 .ipynb_checkpoints\n",
      "4.0K drwxrwxrwx 3 1000 1000 4.0K Sep  1 04:35 .\n",
      "4.0K drwxrwxrwx 6 1000 1000 4.0K Sep  1 05:27 ..\n",
      "day_0 exists, OK\n",
      "day_1 exists, OK\n",
      "day_2 exists, OK\n",
      "day_3 exists, OK\n",
      "day_4 exists, OK\n",
      "day_5 exists, OK\n",
      "day_6 exists, OK\n",
      "day_7 exists, OK\n",
      "day_8 exists, OK\n",
      "day_9 exists, OK\n",
      "day_10 exists, OK\n",
      "day_11 exists, OK\n",
      "day_12 exists, OK\n",
      "day_13 exists, OK\n",
      "day_14 exists, OK\n",
      "day_15 exists, OK\n",
      "day_16 exists, OK\n",
      "day_17 exists, OK\n",
      "day_18 exists, OK\n",
      "day_19 exists, OK\n",
      "day_20 exists, OK\n",
      "day_21 exists, OK\n",
      "day_22 exists, OK\n",
      "day_23 exists, OK\n",
      "/workspace/dlrm/notebooks/preproc\n",
      "Criteo data verified\n",
      "Performing spark preprocessing\n",
      "Starting spark standalone\n",
      "starting org.apache.spark.deploy.master.Master, logging to /opt/spark-2.4.5-bin-hadoop2.7/logs/spark--org.apache.spark.deploy.master.Master-1-dev-cub.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark-2.4.5-bin-hadoop2.7/logs/spark--org.apache.spark.deploy.worker.Worker-1-dev-cub.out\n",
      "Generating the dictionary...\n",
      "20/09/01 05:28:24 WARN Utils: Your hostname, dev-cub resolves to a loopback address: 127.0.1.1; using 192.168.1.99 instead (on interface eno1)\n",
      "20/09/01 05:28:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/09/01 05:28:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/09/01 05:28:25 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/09/01 05:28:25 INFO SparkContext: Submitted application: spark_data_utils.py\n",
      "20/09/01 05:28:25 INFO SecurityManager: Changing view acls to: root\n",
      "20/09/01 05:28:25 INFO SecurityManager: Changing modify acls to: root\n",
      "20/09/01 05:28:25 INFO SecurityManager: Changing view acls groups to: \n",
      "20/09/01 05:28:25 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/09/01 05:28:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "20/09/01 05:28:25 INFO Utils: Successfully started service 'sparkDriver' on port 33273.\n",
      "20/09/01 05:28:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/09/01 05:28:25 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/09/01 05:28:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/09/01 05:28:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/09/01 05:28:25 INFO DiskBlockManager: Created local directory at /data/dlrm/spark/tmp/blockmgr-727e10df-f0fa-4efa-be98-57fa77d94901\n",
      "20/09/01 05:28:25 INFO MemoryStore: MemoryStore started with capacity 16.9 GB\n",
      "20/09/01 05:28:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/09/01 05:28:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/09/01 05:28:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.99:4040\n",
      "20/09/01 05:28:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://dev-cub:7077...\n",
      "20/09/01 05:28:25 INFO TransportClientFactory: Successfully created connection to dev-cub/127.0.1.1:7077 after 30 ms (0 ms spent in bootstraps)\n",
      "20/09/01 05:28:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200901052826-0004\n",
      "20/09/01 05:28:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052826-0004/0 on worker-20200901052822-192.168.1.99-38283 (192.168.1.99:38283) with 10 core(s)\n",
      "20/09/01 05:28:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052826-0004/0 on hostPort 192.168.1.99:38283 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052826-0004/1 on worker-20200901050844-192.168.1.99-34631 (192.168.1.99:34631) with 10 core(s)\n",
      "20/09/01 05:28:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052826-0004/1 on hostPort 192.168.1.99:34631 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34161.\n",
      "20/09/01 05:28:26 INFO NettyBlockTransferService: Server created on 192.168.1.99:34161\n",
      "20/09/01 05:28:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/09/01 05:28:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052826-0004/1 is now RUNNING\n",
      "20/09/01 05:28:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.99, 34161, None)\n",
      "20/09/01 05:28:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:34161 with 16.9 GB RAM, BlockManagerId(driver, 192.168.1.99, 34161, None)\n",
      "20/09/01 05:28:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052826-0004/0 is now RUNNING\n",
      "20/09/01 05:28:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.99, 34161, None)\n",
      "20/09/01 05:28:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.99, 34161, None)\n",
      "20/09/01 05:28:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/09/01 05:28:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/workspace/dlrm/notebooks/preproc/spark-warehouse').\n",
      "20/09/01 05:28:26 INFO SharedState: Warehouse path is 'file:/workspace/dlrm/notebooks/preproc/spark-warehouse'.\n",
      "20/09/01 05:28:26 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/09/01 05:28:27 INFO InMemoryFileIndex: It took 79 ms to list leaf files for 24 paths.\n",
      "20/09/01 05:28:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:51942) with ID 0\n",
      "20/09/01 05:28:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:51944) with ID 1\n",
      "20/09/01 05:28:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:44337 with 51.0 GB RAM, BlockManagerId(0, 192.168.1.99, 44337, None)\n",
      "20/09/01 05:28:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:44319 with 51.0 GB RAM, BlockManagerId(1, 192.168.1.99, 44319, None)\n",
      "20/09/01 05:28:29 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/09/01 05:28:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/09/01 05:28:29 INFO FileSourceStrategy: Output Data Schema: struct<_c2: string, _c3: string>\n",
      "20/09/01 05:28:29 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/09/01 05:28:29 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/09/01 05:28:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/09/01 05:28:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/09/01 05:28:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:29 INFO CodeGenerator: Code generated in 171.930613 ms\n",
      "20/09/01 05:28:29 INFO CodeGenerator: Code generated in 39.901971 ms\n",
      "20/09/01 05:28:29 INFO CodeGenerator: Code generated in 54.922292 ms\n",
      "20/09/01 05:28:29 INFO CodeGenerator: Code generated in 8.059205 ms\n",
      "20/09/01 05:28:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 367.9 KB, free 16.9 GB)\n",
      "20/09/01 05:28:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KB, free 16.9 GB)\n",
      "20/09/01 05:28:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.99:34161 (size: 23.6 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:30 INFO SparkContext: Created broadcast 0 from parquet at NativeMethodAccessorImpl.java:0\n",
      "20/09/01 05:28:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 28059930 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/09/01 05:28:30 INFO CodeGenerator: Code generated in 8.364117 ms\n",
      "20/09/01 05:28:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Registering RDD 4 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 48 output partitions\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/09/01 05:28:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.9 KB, free 16.9 GB)\n",
      "20/09/01 05:28:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.8 KB, free 16.9 GB)\n",
      "20/09/01 05:28:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.99:34161 (size: 13.8 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:30 INFO DAGScheduler: Submitting 24 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "20/09/01 05:28:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 24 tasks\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.99, executor 1, partition 0, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.1.99, executor 0, partition 1, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 192.168.1.99, executor 1, partition 2, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 192.168.1.99, executor 0, partition 3, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 192.168.1.99, executor 1, partition 4, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 192.168.1.99, executor 0, partition 5, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 192.168.1.99, executor 1, partition 6, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 192.168.1.99, executor 0, partition 7, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, 192.168.1.99, executor 1, partition 8, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, 192.168.1.99, executor 0, partition 9, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, 192.168.1.99, executor 1, partition 10, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, 192.168.1.99, executor 0, partition 11, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, 192.168.1.99, executor 1, partition 12, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, 192.168.1.99, executor 0, partition 13, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, 192.168.1.99, executor 1, partition 14, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, 192.168.1.99, executor 0, partition 15, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, 192.168.1.99, executor 1, partition 16, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, 192.168.1.99, executor 0, partition 17, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, 192.168.1.99, executor 1, partition 18, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:30 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, 192.168.1.99, executor 0, partition 19, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.99:44319 (size: 13.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.99:44337 (size: 13.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.99:44319 (size: 23.6 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.99:44337 (size: 23.6 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, 192.168.1.99, executor 0, partition 20, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, 192.168.1.99, executor 1, partition 21, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, 192.168.1.99, executor 0, partition 22, PROCESS_LOCAL, 8239 bytes)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, 192.168.1.99, executor 1, partition 23, PROCESS_LOCAL, 8240 bytes)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 8568 ms on 192.168.1.99 (executor 0) (1/24)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 8579 ms on 192.168.1.99 (executor 1) (2/24)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 8578 ms on 192.168.1.99 (executor 0) (3/24)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 8581 ms on 192.168.1.99 (executor 1) (4/24)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 8581 ms on 192.168.1.99 (executor 1) (5/24)\n",
      "20/09/01 05:28:38 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 8581 ms on 192.168.1.99 (executor 0) (6/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 8650 ms on 192.168.1.99 (executor 0) (7/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 8684 ms on 192.168.1.99 (executor 1) (8/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 8688 ms on 192.168.1.99 (executor 0) (9/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 8720 ms on 192.168.1.99 (executor 0) (10/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8725 ms on 192.168.1.99 (executor 0) (11/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 8725 ms on 192.168.1.99 (executor 1) (12/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 8740 ms on 192.168.1.99 (executor 0) (13/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 8752 ms on 192.168.1.99 (executor 0) (14/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 8812 ms on 192.168.1.99 (executor 0) (15/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8884 ms on 192.168.1.99 (executor 1) (16/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 8888 ms on 192.168.1.99 (executor 1) (17/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 8894 ms on 192.168.1.99 (executor 1) (18/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 8900 ms on 192.168.1.99 (executor 1) (19/24)\n",
      "20/09/01 05:28:39 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 9133 ms on 192.168.1.99 (executor 1) (20/24)\n",
      "20/09/01 05:28:40 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 1196 ms on 192.168.1.99 (executor 1) (21/24)\n",
      "20/09/01 05:28:41 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 3017 ms on 192.168.1.99 (executor 0) (22/24)\n",
      "20/09/01 05:28:41 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 3041 ms on 192.168.1.99 (executor 1) (23/24)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 3086 ms on 192.168.1.99 (executor 0) (24/24)\n",
      "20/09/01 05:28:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:42 INFO DAGScheduler: ShuffleMapStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 11.668 s\n",
      "20/09/01 05:28:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "20/09/01 05:28:42 INFO DAGScheduler: running: Set()\n",
      "20/09/01 05:28:42 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "20/09/01 05:28:42 INFO DAGScheduler: failed: Set()\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/09/01 05:28:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 29.1 KB, free 16.9 GB)\n",
      "20/09/01 05:28:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.3 KB, free 16.9 GB)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.99:34161 (size: 13.3 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Submitting 48 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "20/09/01 05:28:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 48 tasks\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 24, 192.168.1.99, executor 1, partition 0, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 25, 192.168.1.99, executor 0, partition 1, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 26, 192.168.1.99, executor 1, partition 2, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 27, 192.168.1.99, executor 0, partition 3, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 28, 192.168.1.99, executor 1, partition 4, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 29, 192.168.1.99, executor 0, partition 5, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 30, 192.168.1.99, executor 1, partition 6, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 31, 192.168.1.99, executor 0, partition 7, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 32, 192.168.1.99, executor 1, partition 8, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 33, 192.168.1.99, executor 0, partition 9, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 34, 192.168.1.99, executor 1, partition 10, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 35, 192.168.1.99, executor 0, partition 11, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 36, 192.168.1.99, executor 1, partition 12, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 37, 192.168.1.99, executor 0, partition 13, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 38, 192.168.1.99, executor 1, partition 14, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 39, 192.168.1.99, executor 0, partition 15, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 40, 192.168.1.99, executor 1, partition 16, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 41, 192.168.1.99, executor 0, partition 17, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 42, 192.168.1.99, executor 1, partition 18, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 43, 192.168.1.99, executor 0, partition 19, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.99:44319 (size: 13.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.99:44337 (size: 13.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.99:51944\n",
      "20/09/01 05:28:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.99:51942\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 44, 192.168.1.99, executor 1, partition 20, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 24) in 349 ms on 192.168.1.99 (executor 1) (1/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 45, 192.168.1.99, executor 1, partition 21, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 26) in 383 ms on 192.168.1.99 (executor 1) (2/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 46, 192.168.1.99, executor 1, partition 22, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 32) in 384 ms on 192.168.1.99 (executor 1) (3/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 47, 192.168.1.99, executor 1, partition 23, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 48, 192.168.1.99, executor 1, partition 24, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 34) in 408 ms on 192.168.1.99 (executor 1) (4/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 49, 192.168.1.99, executor 1, partition 25, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 40) in 407 ms on 192.168.1.99 (executor 1) (5/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 38) in 409 ms on 192.168.1.99 (executor 1) (6/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 50, 192.168.1.99, executor 1, partition 26, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 51, 192.168.1.99, executor 1, partition 27, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 36) in 417 ms on 192.168.1.99 (executor 1) (7/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 42) in 415 ms on 192.168.1.99 (executor 1) (8/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 52, 192.168.1.99, executor 0, partition 28, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 53, 192.168.1.99, executor 0, partition 29, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 54, 192.168.1.99, executor 0, partition 30, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 55, 192.168.1.99, executor 1, partition 31, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 56, 192.168.1.99, executor 0, partition 32, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 57, 192.168.1.99, executor 0, partition 33, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 58, 192.168.1.99, executor 0, partition 34, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 59, 192.168.1.99, executor 0, partition 35, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 30) in 437 ms on 192.168.1.99 (executor 1) (9/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 43) in 433 ms on 192.168.1.99 (executor 0) (10/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 25) in 450 ms on 192.168.1.99 (executor 0) (11/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 29) in 449 ms on 192.168.1.99 (executor 0) (12/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 35) in 448 ms on 192.168.1.99 (executor 0) (13/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 39) in 446 ms on 192.168.1.99 (executor 0) (14/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 33) in 449 ms on 192.168.1.99 (executor 0) (15/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 27) in 452 ms on 192.168.1.99 (executor 0) (16/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 60, 192.168.1.99, executor 1, partition 36, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 61, 192.168.1.99, executor 0, partition 37, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 62, 192.168.1.99, executor 0, partition 38, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 63, 192.168.1.99, executor 0, partition 39, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 31) in 488 ms on 192.168.1.99 (executor 0) (17/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 41) in 485 ms on 192.168.1.99 (executor 0) (18/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 28) in 490 ms on 192.168.1.99 (executor 1) (19/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 37) in 488 ms on 192.168.1.99 (executor 0) (20/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 64, 192.168.1.99, executor 1, partition 40, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 44) in 193 ms on 192.168.1.99 (executor 1) (21/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 65, 192.168.1.99, executor 1, partition 41, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 45) in 162 ms on 192.168.1.99 (executor 1) (22/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 66, 192.168.1.99, executor 1, partition 42, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 46) in 177 ms on 192.168.1.99 (executor 1) (23/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 67, 192.168.1.99, executor 1, partition 43, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 47) in 183 ms on 192.168.1.99 (executor 1) (24/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 68, 192.168.1.99, executor 0, partition 44, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 54) in 176 ms on 192.168.1.99 (executor 0) (25/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 69, 192.168.1.99, executor 0, partition 45, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 52) in 194 ms on 192.168.1.99 (executor 0) (26/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 70, 192.168.1.99, executor 1, partition 46, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 71, 192.168.1.99, executor 1, partition 47, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 48) in 231 ms on 192.168.1.99 (executor 1) (27/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 49) in 232 ms on 192.168.1.99 (executor 1) (28/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 58) in 208 ms on 192.168.1.99 (executor 0) (29/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 57) in 230 ms on 192.168.1.99 (executor 0) (30/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 56) in 233 ms on 192.168.1.99 (executor 0) (31/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 55) in 249 ms on 192.168.1.99 (executor 1) (32/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 53) in 251 ms on 192.168.1.99 (executor 0) (33/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 59) in 249 ms on 192.168.1.99 (executor 0) (34/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 50) in 273 ms on 192.168.1.99 (executor 1) (35/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 51) in 272 ms on 192.168.1.99 (executor 1) (36/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 62) in 207 ms on 192.168.1.99 (executor 0) (37/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 61) in 216 ms on 192.168.1.99 (executor 0) (38/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 60) in 227 ms on 192.168.1.99 (executor 1) (39/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 69) in 96 ms on 192.168.1.99 (executor 0) (40/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 68) in 113 ms on 192.168.1.99 (executor 0) (41/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 63) in 231 ms on 192.168.1.99 (executor 0) (42/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 64) in 194 ms on 192.168.1.99 (executor 1) (43/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 67) in 146 ms on 192.168.1.99 (executor 1) (44/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 71) in 107 ms on 192.168.1.99 (executor 1) (45/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 65) in 203 ms on 192.168.1.99 (executor 1) (46/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 66) in 186 ms on 192.168.1.99 (executor 1) (47/48)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 70) in 113 ms on 192.168.1.99 (executor 1) (48/48)\n",
      "20/09/01 05:28:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:42 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.767 s\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 12.505965 s\n",
      "20/09/01 05:28:42 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Registering RDD 10 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 48 output partitions\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/09/01 05:28:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 33.1 KB, free 16.9 GB)\n",
      "20/09/01 05:28:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.8 KB, free 16.9 GB)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.99:34161 (size: 14.8 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:42 INFO DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "20/09/01 05:28:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 48 tasks\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 72, 192.168.1.99, executor 0, partition 0, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 73, 192.168.1.99, executor 1, partition 1, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 74, 192.168.1.99, executor 0, partition 2, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 75, 192.168.1.99, executor 1, partition 3, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 76, 192.168.1.99, executor 0, partition 4, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 77, 192.168.1.99, executor 1, partition 5, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 78, 192.168.1.99, executor 0, partition 6, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 79, 192.168.1.99, executor 1, partition 7, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 80, 192.168.1.99, executor 0, partition 8, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 81, 192.168.1.99, executor 1, partition 9, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 82, 192.168.1.99, executor 0, partition 10, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 83, 192.168.1.99, executor 1, partition 11, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 84, 192.168.1.99, executor 0, partition 12, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 85, 192.168.1.99, executor 1, partition 13, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 86, 192.168.1.99, executor 0, partition 14, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 87, 192.168.1.99, executor 1, partition 15, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 88, 192.168.1.99, executor 0, partition 16, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 89, 192.168.1.99, executor 1, partition 17, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 90, 192.168.1.99, executor 0, partition 18, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 91, 192.168.1.99, executor 1, partition 19, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.99:44337 (size: 14.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.99:44319 (size: 14.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 92, 192.168.1.99, executor 1, partition 20, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 85) in 191 ms on 192.168.1.99 (executor 1) (1/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 93, 192.168.1.99, executor 1, partition 21, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 94, 192.168.1.99, executor 1, partition 22, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 83) in 225 ms on 192.168.1.99 (executor 1) (2/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 75) in 227 ms on 192.168.1.99 (executor 1) (3/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 95, 192.168.1.99, executor 0, partition 23, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 80) in 244 ms on 192.168.1.99 (executor 0) (4/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 96, 192.168.1.99, executor 0, partition 24, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 82) in 249 ms on 192.168.1.99 (executor 0) (5/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 97, 192.168.1.99, executor 0, partition 25, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 88) in 248 ms on 192.168.1.99 (executor 0) (6/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 98, 192.168.1.99, executor 0, partition 26, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 72) in 253 ms on 192.168.1.99 (executor 0) (7/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 99, 192.168.1.99, executor 1, partition 27, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 100, 192.168.1.99, executor 1, partition 28, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 101, 192.168.1.99, executor 1, partition 29, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 89) in 261 ms on 192.168.1.99 (executor 1) (8/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 81) in 263 ms on 192.168.1.99 (executor 1) (9/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 73) in 265 ms on 192.168.1.99 (executor 1) (10/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 102, 192.168.1.99, executor 0, partition 30, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 78) in 267 ms on 192.168.1.99 (executor 0) (11/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 103, 192.168.1.99, executor 0, partition 31, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 84) in 267 ms on 192.168.1.99 (executor 0) (12/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 104, 192.168.1.99, executor 0, partition 32, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 90) in 288 ms on 192.168.1.99 (executor 0) (13/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 105, 192.168.1.99, executor 0, partition 33, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 74) in 292 ms on 192.168.1.99 (executor 0) (14/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 106, 192.168.1.99, executor 0, partition 34, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 86) in 291 ms on 192.168.1.99 (executor 0) (15/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 107, 192.168.1.99, executor 1, partition 35, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 77) in 312 ms on 192.168.1.99 (executor 1) (16/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 108, 192.168.1.99, executor 1, partition 36, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 79) in 317 ms on 192.168.1.99 (executor 1) (17/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 109, 192.168.1.99, executor 1, partition 37, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 91) in 317 ms on 192.168.1.99 (executor 1) (18/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 110, 192.168.1.99, executor 1, partition 38, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 87) in 333 ms on 192.168.1.99 (executor 1) (19/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 111, 192.168.1.99, executor 1, partition 39, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 94) in 149 ms on 192.168.1.99 (executor 1) (20/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 112, 192.168.1.99, executor 1, partition 40, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 92) in 189 ms on 192.168.1.99 (executor 1) (21/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 113, 192.168.1.99, executor 0, partition 41, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 76) in 397 ms on 192.168.1.99 (executor 0) (22/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 114, 192.168.1.99, executor 0, partition 42, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 97) in 194 ms on 192.168.1.99 (executor 0) (23/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 115, 192.168.1.99, executor 0, partition 43, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 98) in 195 ms on 192.168.1.99 (executor 0) (24/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 116, 192.168.1.99, executor 0, partition 44, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 95) in 210 ms on 192.168.1.99 (executor 0) (25/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 117, 192.168.1.99, executor 0, partition 45, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 102) in 190 ms on 192.168.1.99 (executor 0) (26/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 118, 192.168.1.99, executor 0, partition 46, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 103) in 189 ms on 192.168.1.99 (executor 0) (27/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 119, 192.168.1.99, executor 1, partition 47, NODE_LOCAL, 7760 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 99) in 208 ms on 192.168.1.99 (executor 1) (28/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 96) in 238 ms on 192.168.1.99 (executor 0) (29/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 105) in 200 ms on 192.168.1.99 (executor 0) (30/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 101) in 237 ms on 192.168.1.99 (executor 1) (31/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 93) in 281 ms on 192.168.1.99 (executor 1) (32/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 107) in 189 ms on 192.168.1.99 (executor 1) (33/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 100) in 239 ms on 192.168.1.99 (executor 1) (34/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 111) in 130 ms on 192.168.1.99 (executor 1) (35/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 110) in 168 ms on 192.168.1.99 (executor 1) (36/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 112) in 126 ms on 192.168.1.99 (executor 1) (37/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 109) in 189 ms on 192.168.1.99 (executor 1) (38/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 108) in 194 ms on 192.168.1.99 (executor 1) (39/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 104) in 227 ms on 192.168.1.99 (executor 0) (40/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 106) in 229 ms on 192.168.1.99 (executor 0) (41/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 113) in 143 ms on 192.168.1.99 (executor 0) (42/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 115) in 94 ms on 192.168.1.99 (executor 0) (43/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 114) in 102 ms on 192.168.1.99 (executor 0) (44/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 119) in 81 ms on 192.168.1.99 (executor 1) (45/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 118) in 93 ms on 192.168.1.99 (executor 0) (46/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 117) in 94 ms on 192.168.1.99 (executor 0) (47/48)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 116) in 98 ms on 192.168.1.99 (executor 0) (48/48)\n",
      "20/09/01 05:28:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:43 INFO DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.565 s\n",
      "20/09/01 05:28:43 INFO DAGScheduler: looking for newly runnable stages\n",
      "20/09/01 05:28:43 INFO DAGScheduler: running: Set()\n",
      "20/09/01 05:28:43 INFO DAGScheduler: waiting: Set(ResultStage 4)\n",
      "20/09/01 05:28:43 INFO DAGScheduler: failed: Set()\n",
      "20/09/01 05:28:43 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/09/01 05:28:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 168.4 KB, free 16.9 GB)\n",
      "20/09/01 05:28:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 61.9 KB, free 16.9 GB)\n",
      "20/09/01 05:28:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.99:34161 (size: 61.9 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:43 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:43 INFO DAGScheduler: Submitting 48 missing tasks from ResultStage 4 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "20/09/01 05:28:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 48 tasks\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 120, 192.168.1.99, executor 0, partition 0, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 121, 192.168.1.99, executor 1, partition 1, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 122, 192.168.1.99, executor 0, partition 2, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 123, 192.168.1.99, executor 1, partition 3, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 124, 192.168.1.99, executor 0, partition 4, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 125, 192.168.1.99, executor 1, partition 5, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 126, 192.168.1.99, executor 0, partition 6, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 127, 192.168.1.99, executor 1, partition 7, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 128, 192.168.1.99, executor 0, partition 8, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 129, 192.168.1.99, executor 1, partition 9, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 130, 192.168.1.99, executor 0, partition 10, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 131, 192.168.1.99, executor 1, partition 11, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 132, 192.168.1.99, executor 0, partition 12, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 133, 192.168.1.99, executor 1, partition 13, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 134, 192.168.1.99, executor 0, partition 14, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 135, 192.168.1.99, executor 1, partition 15, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 136, 192.168.1.99, executor 0, partition 16, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 137, 192.168.1.99, executor 1, partition 17, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 138, 192.168.1.99, executor 0, partition 18, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 139, 192.168.1.99, executor 1, partition 19, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.99:44337 (size: 61.9 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.99:44319 (size: 61.9 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.99:51942\n",
      "20/09/01 05:28:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.99:51944\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 140, 192.168.1.99, executor 1, partition 20, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 141, 192.168.1.99, executor 1, partition 21, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 142, 192.168.1.99, executor 1, partition 22, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 143, 192.168.1.99, executor 1, partition 23, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 144, 192.168.1.99, executor 1, partition 24, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 145, 192.168.1.99, executor 1, partition 25, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 146, 192.168.1.99, executor 1, partition 26, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 147, 192.168.1.99, executor 1, partition 27, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 148, 192.168.1.99, executor 1, partition 28, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 149, 192.168.1.99, executor 1, partition 29, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 131) in 981 ms on 192.168.1.99 (executor 1) (1/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 137) in 981 ms on 192.168.1.99 (executor 1) (2/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 121) in 983 ms on 192.168.1.99 (executor 1) (3/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 129) in 982 ms on 192.168.1.99 (executor 1) (4/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 139) in 981 ms on 192.168.1.99 (executor 1) (5/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 123) in 984 ms on 192.168.1.99 (executor 1) (6/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 127) in 984 ms on 192.168.1.99 (executor 1) (7/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 125) in 986 ms on 192.168.1.99 (executor 1) (8/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 135) in 986 ms on 192.168.1.99 (executor 1) (9/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 133) in 986 ms on 192.168.1.99 (executor 1) (10/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 150, 192.168.1.99, executor 0, partition 30, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 136) in 1088 ms on 192.168.1.99 (executor 0) (11/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 151, 192.168.1.99, executor 0, partition 31, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 152, 192.168.1.99, executor 0, partition 32, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 153, 192.168.1.99, executor 0, partition 33, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 154, 192.168.1.99, executor 0, partition 34, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 120) in 1094 ms on 192.168.1.99 (executor 0) (12/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 124) in 1093 ms on 192.168.1.99 (executor 0) (13/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 130) in 1092 ms on 192.168.1.99 (executor 0) (14/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 122) in 1093 ms on 192.168.1.99 (executor 0) (15/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 155, 192.168.1.99, executor 0, partition 35, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 156, 192.168.1.99, executor 0, partition 36, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 157, 192.168.1.99, executor 0, partition 37, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 158, 192.168.1.99, executor 0, partition 38, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 126) in 1109 ms on 192.168.1.99 (executor 0) (16/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 159, 192.168.1.99, executor 0, partition 39, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 134) in 1114 ms on 192.168.1.99 (executor 0) (17/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 138) in 1116 ms on 192.168.1.99 (executor 0) (18/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 132) in 1117 ms on 192.168.1.99 (executor 0) (19/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 128) in 1123 ms on 192.168.1.99 (executor 0) (20/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 160, 192.168.1.99, executor 1, partition 40, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 161, 192.168.1.99, executor 1, partition 41, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 141) in 208 ms on 192.168.1.99 (executor 1) (21/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 162, 192.168.1.99, executor 1, partition 42, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 142) in 210 ms on 192.168.1.99 (executor 1) (22/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 140) in 211 ms on 192.168.1.99 (executor 1) (23/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 163, 192.168.1.99, executor 1, partition 43, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 147) in 212 ms on 192.168.1.99 (executor 1) (24/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 164, 192.168.1.99, executor 1, partition 44, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 143) in 230 ms on 192.168.1.99 (executor 1) (25/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 165, 192.168.1.99, executor 1, partition 45, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 145) in 237 ms on 192.168.1.99 (executor 1) (26/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 166, 192.168.1.99, executor 1, partition 46, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 167, 192.168.1.99, executor 1, partition 47, NODE_LOCAL, 7771 bytes)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 144) in 265 ms on 192.168.1.99 (executor 1) (27/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 148) in 256 ms on 192.168.1.99 (executor 1) (28/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 146) in 268 ms on 192.168.1.99 (executor 1) (29/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 149) in 259 ms on 192.168.1.99 (executor 1) (30/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 151) in 201 ms on 192.168.1.99 (executor 0) (31/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 152) in 201 ms on 192.168.1.99 (executor 0) (32/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 153) in 209 ms on 192.168.1.99 (executor 0) (33/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 150) in 246 ms on 192.168.1.99 (executor 0) (34/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 160) in 165 ms on 192.168.1.99 (executor 1) (35/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 155) in 263 ms on 192.168.1.99 (executor 0) (36/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 158) in 261 ms on 192.168.1.99 (executor 0) (37/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 157) in 261 ms on 192.168.1.99 (executor 0) (38/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 154) in 275 ms on 192.168.1.99 (executor 0) (39/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 156) in 262 ms on 192.168.1.99 (executor 0) (40/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 162) in 200 ms on 192.168.1.99 (executor 1) (41/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 159) in 266 ms on 192.168.1.99 (executor 0) (42/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 165) in 181 ms on 192.168.1.99 (executor 1) (43/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 167) in 156 ms on 192.168.1.99 (executor 1) (44/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 166) in 158 ms on 192.168.1.99 (executor 1) (45/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 161) in 218 ms on 192.168.1.99 (executor 1) (46/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 164) in 196 ms on 192.168.1.99 (executor 1) (47/48)\n",
      "20/09/01 05:28:44 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 163) in 206 ms on 192.168.1.99 (executor 1) (48/48)\n",
      "20/09/01 05:28:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:44 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.421 s\n",
      "20/09/01 05:28:44 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.010517 s\n",
      "20/09/01 05:28:44 INFO FileFormatWriter: Write Job 9b2d3711-ab2f-4ce0-a22e-2487e92de194 committed.\n",
      "20/09/01 05:28:44 INFO FileFormatWriter: Finished processing stats for write job 9b2d3711-ab2f-4ce0-a22e-2487e92de194.\n",
      "20/09/01 05:28:45 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
      "20/09/01 05:28:45 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Missing parents: List()\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 71.0 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.3 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.99:34161 (size: 25.3 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/09/01 05:28:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 168, 192.168.1.99, executor 0, partition 0, PROCESS_LOCAL, 8094 bytes)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.99:44337 (size: 25.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 168) in 61 ms on 192.168.1.99 (executor 0) (1/1)\n",
      "20/09/01 05:28:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:45 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.075 s\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.077866 s\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 16\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 10\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 14\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 22\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 55\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 94\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 57\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 60\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 59\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 153\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 161\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 46\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 138\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 39\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 64\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 42\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 31\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 146\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 112\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 78\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 139\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 40\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 80\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 33\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 50\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 118\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.99:34161 in memory (size: 25.3 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.99:44337 in memory (size: 25.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 95\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 84\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 89\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 132\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 63\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 117\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 81\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 21\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 29\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 51\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 72\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 109\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 156\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 127\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 45\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.99:34161 in memory (size: 13.3 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.99:44337 in memory (size: 13.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.99:44319 in memory (size: 13.3 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 129\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 68\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 124\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 17\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 26\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 152\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 65\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 38\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 134\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 90\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 108\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 110\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 62\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 126\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 11\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 97\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 159\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.99:34161 in memory (size: 23.6 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.99:44319 in memory (size: 23.6 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.99:44337 in memory (size: 23.6 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 53\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 160\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 135\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 1\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 27\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 155\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 107\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 93\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 98\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 85\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.99:34161 in memory (size: 61.9 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.99:44337 in memory (size: 61.9 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.99:44319 in memory (size: 61.9 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 151\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 9\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 41\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 75\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 2\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 36\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 96\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 144\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 150\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 15\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 92\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 70\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 130\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 71\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.99:34161 in memory (size: 14.8 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.99:44319 in memory (size: 14.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.99:44337 in memory (size: 14.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 106\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 48\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 49\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 88\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 30\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 154\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 73\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 103\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 7\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.99:34161 in memory (size: 13.8 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.99:44319 in memory (size: 13.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.99:44337 in memory (size: 13.8 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 121\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 119\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 86\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 87\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 145\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 47\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 142\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 58\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 25\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 5\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 122\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 123\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 133\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 43\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 77\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 82\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 37\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 116\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 137\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 69\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 143\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 113\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 6\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 18\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 100\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 131\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 101\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 34\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 105\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 120\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 148\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 23\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 91\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 67\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 52\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 149\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 32\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 20\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 74\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 128\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 157\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 24\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 76\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 102\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 28\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 114\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 147\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 4\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 99\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 136\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 44\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 158\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 13\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 19\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 104\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 3\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 79\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 61\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 140\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 54\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 66\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 83\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned shuffle 1\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 141\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 111\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 125\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 35\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 56\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 12\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned shuffle 0\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 8\n",
      "20/09/01 05:28:45 INFO ContextCleaner: Cleaned accumulator 115\n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(column_id#48),isnotnull(part_id#51)\n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Output Data Schema: struct<column_id: int, data: string, count: bigint, part_id: int, mono_id: bigint ... 3 more fields>\n",
      "20/09/01 05:28:45 INFO FileSourceScanExec: Pushed Filters: IsNotNull(column_id),IsNotNull(part_id)\n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(column_id#48)\n",
      "20/09/01 05:28:45 INFO FileSourceStrategy: Output Data Schema: struct<column_id: int, part_id: int, mono_id: bigint ... 1 more fields>\n",
      "20/09/01 05:28:45 INFO FileSourceScanExec: Pushed Filters: IsNotNull(column_id)\n",
      "20/09/01 05:28:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/09/01 05:28:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/09/01 05:28:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/09/01 05:28:45 INFO CodeGenerator: Code generated in 9.863098 ms\n",
      "20/09/01 05:28:45 INFO CodeGenerator: Code generated in 8.364374 ms\n",
      "20/09/01 05:28:45 INFO CodeGenerator: Code generated in 18.580329 ms\n",
      "20/09/01 05:28:45 INFO CodeGenerator: Code generated in 37.041467 ms\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 371.8 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.99:34161 (size: 24.2 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO SparkContext: Created broadcast 6 from run at ThreadPoolExecutor.java:1149\n",
      "20/09/01 05:28:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4234079 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/09/01 05:28:45 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1149\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Registering RDD 19 (run at ThreadPoolExecutor.java:1149) as input to shuffle 2\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Registering RDD 22 (run at ThreadPoolExecutor.java:1149) as input to shuffle 3\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Got job 3 (run at ThreadPoolExecutor.java:1149) with 48 output partitions\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Final stage: ResultStage 8 (run at ThreadPoolExecutor.java:1149)\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[19] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 30.7 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.0 KB, free 16.9 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.99:34161 (size: 13.0 KB, free: 16.9 GB)\n",
      "20/09/01 05:28:45 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\n",
      "20/09/01 05:28:45 INFO DAGScheduler: Submitting 20 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[19] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "20/09/01 05:28:45 INFO TaskSchedulerImpl: Adding task set 6.0 with 20 tasks\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 169, 192.168.1.99, executor 1, partition 0, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 170, 192.168.1.99, executor 0, partition 1, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 171, 192.168.1.99, executor 1, partition 2, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 172, 192.168.1.99, executor 0, partition 3, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 173, 192.168.1.99, executor 1, partition 4, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 174, 192.168.1.99, executor 0, partition 5, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 175, 192.168.1.99, executor 1, partition 6, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 176, 192.168.1.99, executor 0, partition 7, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 177, 192.168.1.99, executor 1, partition 8, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 178, 192.168.1.99, executor 0, partition 9, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 179, 192.168.1.99, executor 1, partition 10, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 180, 192.168.1.99, executor 0, partition 11, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 181, 192.168.1.99, executor 1, partition 12, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 182, 192.168.1.99, executor 0, partition 13, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 183, 192.168.1.99, executor 1, partition 14, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 184, 192.168.1.99, executor 0, partition 15, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 185, 192.168.1.99, executor 1, partition 16, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 186, 192.168.1.99, executor 0, partition 17, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 187, 192.168.1.99, executor 1, partition 18, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 188, 192.168.1.99, executor 0, partition 19, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.99:44337 (size: 13.0 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.99:44319 (size: 13.0 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.99:44337 (size: 24.2 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.99:44319 (size: 24.2 KB, free: 51.0 GB)\n",
      "20/09/01 05:28:45 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 169, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00037-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 189, 192.168.1.99, executor 1, partition 0, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 4.0 in stage 6.0 (TID 173, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00000-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 12.0 in stage 6.0 (TID 181, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00002-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 6.0 in stage 6.0 (TID 175, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 2.0 in stage 6.0 (TID 171, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00039-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 16.0 in stage 6.0 (TID 185, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00010-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 8.0 in stage 6.0 (TID 177, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00008-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 8.1 in stage 6.0 (TID 190, 192.168.1.99, executor 1, partition 8, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 16.1 in stage 6.0 (TID 191, 192.168.1.99, executor 1, partition 16, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 2.1 in stage 6.0 (TID 192, 192.168.1.99, executor 1, partition 2, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 6.1 in stage 6.0 (TID 193, 192.168.1.99, executor 1, partition 6, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 12.1 in stage 6.0 (TID 194, 192.168.1.99, executor 1, partition 12, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 4.1 in stage 6.0 (TID 195, 192.168.1.99, executor 1, partition 4, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 10.0 in stage 6.0 (TID 179, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00033-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 14.0 in stage 6.0 (TID 183, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00032-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 WARN TaskSetManager: Lost task 18.0 in stage 6.0 (TID 187, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00038-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 18.1 in stage 6.0 (TID 196, 192.168.1.99, executor 1, partition 18, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 14.1 in stage 6.0 (TID 197, 192.168.1.99, executor 1, partition 14, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 10.1 in stage 6.0 (TID 198, 192.168.1.99, executor 1, partition 10, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 0.1 in stage 6.0 (TID 189) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00037-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 0.2 in stage 6.0 (TID 199, 192.168.1.99, executor 1, partition 0, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 12.1 in stage 6.0 (TID 194) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00002-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 12.2 in stage 6.0 (TID 200, 192.168.1.99, executor 1, partition 12, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 8.1 in stage 6.0 (TID 190) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00008-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 8.2 in stage 6.0 (TID 201, 192.168.1.99, executor 1, partition 8, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 6.1 in stage 6.0 (TID 193) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 6.2 in stage 6.0 (TID 202, 192.168.1.99, executor 1, partition 6, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 4.1 in stage 6.0 (TID 195) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00000-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 4.2 in stage 6.0 (TID 203, 192.168.1.99, executor 1, partition 4, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 14.1 in stage 6.0 (TID 197) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00032-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 14.2 in stage 6.0 (TID 204, 192.168.1.99, executor 1, partition 14, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 2.1 in stage 6.0 (TID 192) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00039-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 2.2 in stage 6.0 (TID 205, 192.168.1.99, executor 1, partition 2, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 16.1 in stage 6.0 (TID 191) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00010-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 16.2 in stage 6.0 (TID 206, 192.168.1.99, executor 1, partition 16, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 10.1 in stage 6.0 (TID 198) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00033-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 10.2 in stage 6.0 (TID 207, 192.168.1.99, executor 1, partition 10, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 6.2 in stage 6.0 (TID 202) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 6.3 in stage 6.0 (TID 208, 192.168.1.99, executor 1, partition 6, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 18.1 in stage 6.0 (TID 196) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00038-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 18.2 in stage 6.0 (TID 209, 192.168.1.99, executor 1, partition 18, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 0.2 in stage 6.0 (TID 199) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00037-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 0.3 in stage 6.0 (TID 210, 192.168.1.99, executor 1, partition 0, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 12.2 in stage 6.0 (TID 200) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00002-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 12.3 in stage 6.0 (TID 211, 192.168.1.99, executor 1, partition 12, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 170) in 303 ms on 192.168.1.99 (executor 0) (1/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 172) in 303 ms on 192.168.1.99 (executor 0) (2/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 178) in 309 ms on 192.168.1.99 (executor 0) (3/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 184) in 309 ms on 192.168.1.99 (executor 0) (4/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 188) in 308 ms on 192.168.1.99 (executor 0) (5/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 174) in 310 ms on 192.168.1.99 (executor 0) (6/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 186) in 308 ms on 192.168.1.99 (executor 0) (7/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 180) in 310 ms on 192.168.1.99 (executor 0) (8/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 176) in 311 ms on 192.168.1.99 (executor 0) (9/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 182) in 314 ms on 192.168.1.99 (executor 0) (10/20)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 14.2 in stage 6.0 (TID 204) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00032-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 2.2 in stage 6.0 (TID 205) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00039-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 16.2 in stage 6.0 (TID 206) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00010-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 8.2 in stage 6.0 (TID 201) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00008-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 4.2 in stage 6.0 (TID 203) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00000-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 4.3 in stage 6.0 (TID 212, 192.168.1.99, executor 1, partition 4, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 8.3 in stage 6.0 (TID 213, 192.168.1.99, executor 1, partition 8, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 16.3 in stage 6.0 (TID 214, 192.168.1.99, executor 1, partition 16, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 2.3 in stage 6.0 (TID 215, 192.168.1.99, executor 1, partition 2, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 14.3 in stage 6.0 (TID 216, 192.168.1.99, executor 1, partition 14, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 10.2 in stage 6.0 (TID 207) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00033-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Starting task 10.3 in stage 6.0 (TID 217, 192.168.1.99, executor 1, partition 10, PROCESS_LOCAL, 8334 bytes)\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 6.3 in stage 6.0 (TID 208) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 ERROR TaskSetManager: Task 6 in stage 6.0 failed 4 times; aborting job\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 12.3 in stage 6.0 (TID 211) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00002-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 18.2 in stage 6.0 (TID 209) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00038-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 0.3 in stage 6.0 (TID 210) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00037-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Cancelling stage 6\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Stage 6 was cancelled\n",
      "20/09/01 05:28:46 INFO DAGScheduler: ShuffleMapStage 6 (run at ThreadPoolExecutor.java:1149) failed in 0.352 s due to Job aborted due to stage failure: Task 6 in stage 6.0 failed 4 times, most recent failure: Lost task 6.3 in stage 6.0 (TID 208, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 14.3 in stage 6.0 (TID 216) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00032-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 2.3 in stage 6.0 (TID 215) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00039-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 4.3 in stage 6.0 (TID 212) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00000-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 8.3 in stage 6.0 (TID 213) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00008-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 16.3 in stage 6.0 (TID 214) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00010-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:46 INFO TaskSetManager: Lost task 10.3 in stage 6.0 (TID 217) on 192.168.1.99, executor 1: java.io.FileNotFoundException (File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00033-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]\n",
      "20/09/01 05:28:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/09/01 05:28:46 INFO DAGScheduler: Job 3 failed: run at ThreadPoolExecutor.java:1149, took 0.361380 s\n",
      "20/09/01 05:28:46 ERROR FileFormatWriter: Aborting job 62e45b84-eeda-4268-8331-a19a5f390f8a.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 4 times, most recent failure: Lost task 6.3 in stage 6.0 (TID 208, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\t... 3 more\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 507, in <module>\n",
      "    _main()\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 414, in _main\n",
      "    args.write_mode)\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 284, in save_combined_model\n",
      "    df.write.parquet(path, mode=mode)\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 844, in parquet\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o147.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:87)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:212)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 4 times, most recent failure: Lost task 6.3 in stage 6.0 (TID 208, 192.168.1.99, executor 1): java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: File file:/data/dlrm/spark/output/models/partial_ids.parquet/part-00016-6970cdfe-6bb5-4a73-84a4-0d32fb786064-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\t... 3 more\n",
      "\n",
      "20/09/01 05:28:46 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/09/01 05:28:46 INFO SparkUI: Stopped Spark web UI at http://192.168.1.99:4040\n",
      "20/09/01 05:28:46 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/09/01 05:28:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/09/01 05:28:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/09/01 05:28:46 INFO MemoryStore: MemoryStore cleared\n",
      "20/09/01 05:28:46 INFO BlockManager: BlockManager stopped\n",
      "20/09/01 05:28:46 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/09/01 05:28:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/09/01 05:28:46 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/09/01 05:28:46 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/09/01 05:28:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-2555f7e4-0f21-4821-ba61-eb9d760e2135\n",
      "20/09/01 05:28:46 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-96481884-9d64-42a5-88db-7111e3afe03f/pyspark-3ff26af6-174c-42be-afd7-9dcc3f80e788\n",
      "20/09/01 05:28:46 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-96481884-9d64-42a5-88db-7111e3afe03f\n",
      "Transforming the train data from day_0 to day_22...\n",
      "20/09/01 05:28:47 WARN Utils: Your hostname, dev-cub resolves to a loopback address: 127.0.1.1; using 192.168.1.99 instead (on interface eno1)\n",
      "20/09/01 05:28:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/09/01 05:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/09/01 05:28:48 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/09/01 05:28:48 INFO SparkContext: Submitted application: spark_data_utils.py\n",
      "20/09/01 05:28:48 INFO SecurityManager: Changing view acls to: root\n",
      "20/09/01 05:28:48 INFO SecurityManager: Changing modify acls to: root\n",
      "20/09/01 05:28:48 INFO SecurityManager: Changing view acls groups to: \n",
      "20/09/01 05:28:48 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/09/01 05:28:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "20/09/01 05:28:48 INFO Utils: Successfully started service 'sparkDriver' on port 44431.\n",
      "20/09/01 05:28:48 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/09/01 05:28:48 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/09/01 05:28:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/09/01 05:28:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/09/01 05:28:48 INFO DiskBlockManager: Created local directory at /data/dlrm/spark/tmp/blockmgr-457d05d1-f07c-4249-b977-cb26ccc8a05e\n",
      "20/09/01 05:28:49 INFO MemoryStore: MemoryStore started with capacity 16.9 GB\n",
      "20/09/01 05:28:49 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/09/01 05:28:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/09/01 05:28:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.99:4040\n",
      "20/09/01 05:28:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://dev-cub:7077...\n",
      "20/09/01 05:28:49 INFO TransportClientFactory: Successfully created connection to dev-cub/127.0.1.1:7077 after 33 ms (0 ms spent in bootstraps)\n",
      "20/09/01 05:28:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200901052849-0005\n",
      "20/09/01 05:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052849-0005/0 on worker-20200901052822-192.168.1.99-38283 (192.168.1.99:38283) with 10 core(s)\n",
      "20/09/01 05:28:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44999.\n",
      "20/09/01 05:28:49 INFO NettyBlockTransferService: Server created on 192.168.1.99:44999\n",
      "20/09/01 05:28:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/09/01 05:28:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052849-0005/0 on hostPort 192.168.1.99:38283 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052849-0005/1 on worker-20200901050844-192.168.1.99-34631 (192.168.1.99:34631) with 10 core(s)\n",
      "20/09/01 05:28:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052849-0005/1 on hostPort 192.168.1.99:34631 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052849-0005/1 is now RUNNING\n",
      "20/09/01 05:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052849-0005/0 is now RUNNING\n",
      "20/09/01 05:28:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.99, 44999, None)\n",
      "20/09/01 05:28:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:44999 with 16.9 GB RAM, BlockManagerId(driver, 192.168.1.99, 44999, None)\n",
      "20/09/01 05:28:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.99, 44999, None)\n",
      "20/09/01 05:28:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.99, 44999, None)\n",
      "20/09/01 05:28:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/09/01 05:28:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/workspace/dlrm/notebooks/preproc/spark-warehouse').\n",
      "20/09/01 05:28:49 INFO SharedState: Warehouse path is 'file:/workspace/dlrm/notebooks/preproc/spark-warehouse'.\n",
      "20/09/01 05:28:50 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/09/01 05:28:50 INFO InMemoryFileIndex: It took 75 ms to list leaf files for 23 paths.\n",
      "20/09/01 05:28:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:37948) with ID 1\n",
      "20/09/01 05:28:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:37950) with ID 0\n",
      "20/09/01 05:28:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:36967 with 51.0 GB RAM, BlockManagerId(1, 192.168.1.99, 36967, None)\n",
      "20/09/01 05:28:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:42597 with 51.0 GB RAM, BlockManagerId(0, 192.168.1.99, 42597, None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o44.parquet.\n",
      ": org.apache.spark.sql.AnalysisException: Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 507, in <module>\n",
      "    _main()\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 448, in _main\n",
      "    models = list(load_column_models(spark, args.model_folder, bool(args.model_size_file)))\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 310, in load_column_models\n",
      "    df = spark.read.parquet(path)\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;'\n",
      "20/09/01 05:28:52 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/09/01 05:28:52 INFO SparkUI: Stopped Spark web UI at http://192.168.1.99:4040\n",
      "20/09/01 05:28:52 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/09/01 05:28:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/09/01 05:28:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/09/01 05:28:52 INFO MemoryStore: MemoryStore cleared\n",
      "20/09/01 05:28:52 INFO BlockManager: BlockManager stopped\n",
      "20/09/01 05:28:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/09/01 05:28:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/09/01 05:28:52 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/09/01 05:28:52 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/09/01 05:28:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-737b0ea9-d193-4cb1-9839-fa2b2e899449\n",
      "20/09/01 05:28:52 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-deb10fb8-850a-47ab-b112-366f626f3604/pyspark-3543cf13-d25e-4adb-a437-97d01cfecb9f\n",
      "20/09/01 05:28:52 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-deb10fb8-850a-47ab-b112-366f626f3604\n",
      "Splitting the last day into 2 parts of test and validation...\n",
      "Transforming the test data in day_23...\n",
      "20/09/01 05:28:53 WARN Utils: Your hostname, dev-cub resolves to a loopback address: 127.0.1.1; using 192.168.1.99 instead (on interface eno1)\n",
      "20/09/01 05:28:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/09/01 05:28:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/09/01 05:28:54 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/09/01 05:28:54 INFO SparkContext: Submitted application: spark_data_utils.py\n",
      "20/09/01 05:28:54 INFO SecurityManager: Changing view acls to: root\n",
      "20/09/01 05:28:54 INFO SecurityManager: Changing modify acls to: root\n",
      "20/09/01 05:28:54 INFO SecurityManager: Changing view acls groups to: \n",
      "20/09/01 05:28:54 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/09/01 05:28:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "20/09/01 05:28:54 INFO Utils: Successfully started service 'sparkDriver' on port 44171.\n",
      "20/09/01 05:28:54 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/09/01 05:28:54 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/09/01 05:28:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/09/01 05:28:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/09/01 05:28:54 INFO DiskBlockManager: Created local directory at /data/dlrm/spark/tmp/blockmgr-17f546e4-5416-43a7-8dae-97cda56f48d4\n",
      "20/09/01 05:28:54 INFO MemoryStore: MemoryStore started with capacity 16.9 GB\n",
      "20/09/01 05:28:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/09/01 05:28:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/09/01 05:28:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.99:4040\n",
      "20/09/01 05:28:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://dev-cub:7077...\n",
      "20/09/01 05:28:55 INFO TransportClientFactory: Successfully created connection to dev-cub/127.0.1.1:7077 after 33 ms (0 ms spent in bootstraps)\n",
      "20/09/01 05:28:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200901052855-0006\n",
      "20/09/01 05:28:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052855-0006/0 on worker-20200901052822-192.168.1.99-38283 (192.168.1.99:38283) with 10 core(s)\n",
      "20/09/01 05:28:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052855-0006/0 on hostPort 192.168.1.99:38283 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052855-0006/1 on worker-20200901050844-192.168.1.99-34631 (192.168.1.99:34631) with 10 core(s)\n",
      "20/09/01 05:28:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34509.\n",
      "20/09/01 05:28:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052855-0006/1 on hostPort 192.168.1.99:34631 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:28:55 INFO NettyBlockTransferService: Server created on 192.168.1.99:34509\n",
      "20/09/01 05:28:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/09/01 05:28:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052855-0006/1 is now RUNNING\n",
      "20/09/01 05:28:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052855-0006/0 is now RUNNING\n",
      "20/09/01 05:28:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.99, 34509, None)\n",
      "20/09/01 05:28:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:34509 with 16.9 GB RAM, BlockManagerId(driver, 192.168.1.99, 34509, None)\n",
      "20/09/01 05:28:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.99, 34509, None)\n",
      "20/09/01 05:28:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.99, 34509, None)\n",
      "20/09/01 05:28:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/09/01 05:28:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/workspace/dlrm/notebooks/preproc/spark-warehouse').\n",
      "20/09/01 05:28:55 INFO SharedState: Warehouse path is 'file:/workspace/dlrm/notebooks/preproc/spark-warehouse'.\n",
      "20/09/01 05:28:55 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/09/01 05:28:56 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.\n",
      "20/09/01 05:28:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:39652) with ID 0\n",
      "20/09/01 05:28:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:39654) with ID 1\n",
      "20/09/01 05:28:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:41513 with 51.0 GB RAM, BlockManagerId(0, 192.168.1.99, 41513, None)\n",
      "20/09/01 05:28:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:42347 with 51.0 GB RAM, BlockManagerId(1, 192.168.1.99, 42347, None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o41.parquet.\n",
      ": org.apache.spark.sql.AnalysisException: Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 507, in <module>\n",
      "    _main()\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 448, in _main\n",
      "    models = list(load_column_models(spark, args.model_folder, bool(args.model_size_file)))\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 310, in load_column_models\n",
      "    df = spark.read.parquet(path)\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;'\n",
      "20/09/01 05:28:57 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/09/01 05:28:57 INFO SparkUI: Stopped Spark web UI at http://192.168.1.99:4040\n",
      "20/09/01 05:28:57 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/09/01 05:28:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/09/01 05:28:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/09/01 05:28:57 INFO MemoryStore: MemoryStore cleared\n",
      "20/09/01 05:28:57 INFO BlockManager: BlockManager stopped\n",
      "20/09/01 05:28:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/09/01 05:28:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/09/01 05:28:57 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/09/01 05:28:57 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/09/01 05:28:57 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-9f827067-20f8-4d9c-bd88-1ef4db13997f\n",
      "20/09/01 05:28:57 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-9f827067-20f8-4d9c-bd88-1ef4db13997f/pyspark-1d35a0dd-dfb2-40af-98b1-151f116abafb\n",
      "20/09/01 05:28:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-a20491ec-e436-4aea-9dff-365df94e10ba\n",
      "Transforming the validation data in day_23...\n",
      "20/09/01 05:28:58 WARN Utils: Your hostname, dev-cub resolves to a loopback address: 127.0.1.1; using 192.168.1.99 instead (on interface eno1)\n",
      "20/09/01 05:28:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/09/01 05:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/09/01 05:28:59 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/09/01 05:28:59 INFO SparkContext: Submitted application: spark_data_utils.py\n",
      "20/09/01 05:28:59 INFO SecurityManager: Changing view acls to: root\n",
      "20/09/01 05:28:59 INFO SecurityManager: Changing modify acls to: root\n",
      "20/09/01 05:28:59 INFO SecurityManager: Changing view acls groups to: \n",
      "20/09/01 05:28:59 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/09/01 05:28:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "20/09/01 05:29:00 INFO Utils: Successfully started service 'sparkDriver' on port 32781.\n",
      "20/09/01 05:29:00 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/09/01 05:29:00 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/09/01 05:29:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/09/01 05:29:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/09/01 05:29:00 INFO DiskBlockManager: Created local directory at /data/dlrm/spark/tmp/blockmgr-6840b942-dd16-41a5-bb50-1f71caba02ec\n",
      "20/09/01 05:29:00 INFO MemoryStore: MemoryStore started with capacity 16.9 GB\n",
      "20/09/01 05:29:00 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/09/01 05:29:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/09/01 05:29:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.99:4040\n",
      "20/09/01 05:29:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://dev-cub:7077...\n",
      "20/09/01 05:29:00 INFO TransportClientFactory: Successfully created connection to dev-cub/127.0.1.1:7077 after 28 ms (0 ms spent in bootstraps)\n",
      "20/09/01 05:29:00 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200901052900-0007\n",
      "20/09/01 05:29:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052900-0007/0 on worker-20200901052822-192.168.1.99-38283 (192.168.1.99:38283) with 10 core(s)\n",
      "20/09/01 05:29:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052900-0007/0 on hostPort 192.168.1.99:38283 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:29:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200901052900-0007/1 on worker-20200901050844-192.168.1.99-34631 (192.168.1.99:34631) with 10 core(s)\n",
      "20/09/01 05:29:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20200901052900-0007/1 on hostPort 192.168.1.99:34631 with 10 core(s), 96.0 GB RAM\n",
      "20/09/01 05:29:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37695.\n",
      "20/09/01 05:29:00 INFO NettyBlockTransferService: Server created on 192.168.1.99:37695\n",
      "20/09/01 05:29:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/09/01 05:29:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052900-0007/1 is now RUNNING\n",
      "20/09/01 05:29:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200901052900-0007/0 is now RUNNING\n",
      "20/09/01 05:29:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.99, 37695, None)\n",
      "20/09/01 05:29:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:37695 with 16.9 GB RAM, BlockManagerId(driver, 192.168.1.99, 37695, None)\n",
      "20/09/01 05:29:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.99, 37695, None)\n",
      "20/09/01 05:29:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.99, 37695, None)\n",
      "20/09/01 05:29:00 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/09/01 05:29:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/workspace/dlrm/notebooks/preproc/spark-warehouse').\n",
      "20/09/01 05:29:01 INFO SharedState: Warehouse path is 'file:/workspace/dlrm/notebooks/preproc/spark-warehouse'.\n",
      "20/09/01 05:29:01 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/09/01 05:29:01 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.\n",
      "20/09/01 05:29:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:46978) with ID 0\n",
      "20/09/01 05:29:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.99:46980) with ID 1\n",
      "20/09/01 05:29:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:38599 with 51.0 GB RAM, BlockManagerId(0, 192.168.1.99, 38599, None)\n",
      "20/09/01 05:29:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.99:44263 with 51.0 GB RAM, BlockManagerId(1, 192.168.1.99, 44263, None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o41.parquet.\n",
      ": org.apache.spark.sql.AnalysisException: Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 507, in <module>\n",
      "    _main()\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 448, in _main\n",
      "    models = list(load_column_models(spark, args.model_folder, bool(args.model_size_file)))\n",
      "  File \"/workspace/dlrm/notebooks/preproc/spark_data_utils.py\", line 310, in load_column_models\n",
      "    df = spark.read.parquet(path)\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/opt/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: file:/data/dlrm/spark/output/models/2.parquet;'\n",
      "20/09/01 05:29:03 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/09/01 05:29:03 INFO SparkUI: Stopped Spark web UI at http://192.168.1.99:4040\n",
      "20/09/01 05:29:03 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/09/01 05:29:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/09/01 05:29:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/09/01 05:29:03 INFO MemoryStore: MemoryStore cleared\n",
      "20/09/01 05:29:03 INFO BlockManager: BlockManager stopped\n",
      "20/09/01 05:29:03 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/09/01 05:29:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/09/01 05:29:03 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/09/01 05:29:03 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/09/01 05:29:03 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-8a04154c-2221-4798-85e9-a35cf29ee059/pyspark-1d13cadc-219e-478f-afee-2479664c3e06\n",
      "20/09/01 05:29:03 INFO ShutdownHookManager: Deleting directory /data/dlrm/spark/tmp/spark-8a04154c-2221-4798-85e9-a35cf29ee059\n",
      "20/09/01 05:29:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-4770ddf3-4ef9-48e7-a9e9-cc078ea6d8e6\n",
      "Performing final conversion to a custom data format\n",
      "Processing train files...\n",
      "Train files conversion done\n",
      "Processing test files...\n",
      "Test files conversion done\n",
      "Processing validation files...\n",
      "Validation files conversion done\n",
      "Concatenating train files\n",
      "Concatenating test files\n",
      "Concatenating validation files\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "java: no process found\n",
      "+ ls -ltrash\n",
      "+ download_dir=/data/dlrm/criteo\n",
      "+ ./verify_criteo_downloaded.sh /data/dlrm/criteo\n",
      "++ download_dir=/data/dlrm/criteo\n",
      "++ cd /data/dlrm/criteo\n",
      "+++ seq 0 23\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_0\n",
      "++ '[' -f day_0 ']'\n",
      "++ echo 'day_0 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_1\n",
      "++ '[' -f day_1 ']'\n",
      "++ echo 'day_1 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_2\n",
      "++ '[' -f day_2 ']'\n",
      "++ echo 'day_2 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_3\n",
      "++ '[' -f day_3 ']'\n",
      "++ echo 'day_3 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_4\n",
      "++ '[' -f day_4 ']'\n",
      "++ echo 'day_4 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_5\n",
      "++ '[' -f day_5 ']'\n",
      "++ echo 'day_5 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_6\n",
      "++ '[' -f day_6 ']'\n",
      "++ echo 'day_6 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_7\n",
      "++ '[' -f day_7 ']'\n",
      "++ echo 'day_7 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_8\n",
      "++ '[' -f day_8 ']'\n",
      "++ echo 'day_8 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_9\n",
      "++ '[' -f day_9 ']'\n",
      "++ echo 'day_9 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_10\n",
      "++ '[' -f day_10 ']'\n",
      "++ echo 'day_10 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_11\n",
      "++ '[' -f day_11 ']'\n",
      "++ echo 'day_11 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_12\n",
      "++ '[' -f day_12 ']'\n",
      "++ echo 'day_12 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_13\n",
      "++ '[' -f day_13 ']'\n",
      "++ echo 'day_13 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_14\n",
      "++ '[' -f day_14 ']'\n",
      "++ echo 'day_14 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_15\n",
      "++ '[' -f day_15 ']'\n",
      "++ echo 'day_15 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_16\n",
      "++ '[' -f day_16 ']'\n",
      "++ echo 'day_16 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_17\n",
      "++ '[' -f day_17 ']'\n",
      "++ echo 'day_17 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_18\n",
      "++ '[' -f day_18 ']'\n",
      "++ echo 'day_18 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_19\n",
      "++ '[' -f day_19 ']'\n",
      "++ echo 'day_19 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_20\n",
      "++ '[' -f day_20 ']'\n",
      "++ echo 'day_20 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_21\n",
      "++ '[' -f day_21 ']'\n",
      "++ echo 'day_21 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_22\n",
      "++ '[' -f day_22 ']'\n",
      "++ echo 'day_22 exists, OK'\n",
      "++ for i in $(seq 0 23)\n",
      "++ filename=day_23\n",
      "++ '[' -f day_23 ']'\n",
      "++ echo 'day_23 exists, OK'\n",
      "++ cd -\n",
      "++ echo 'Criteo data verified'\n",
      "+ spark_output_path=/data/dlrm/spark/output\n",
      "+ '[' -f /data/dlrm/spark/output/train/_SUCCESS ']'\n",
      "+ echo 'Performing spark preprocessing'\n",
      "+ ./run_spark.sh /data/dlrm/criteo /data/dlrm/spark/output\n",
      "+ conversion_intermediate_dir=/data/dlrm/intermediate_binary\n",
      "+ final_output_dir=/data/dlrm/binary_dataset\n",
      "+ '[' -d /data/dlrm/binary_dataset/train ']'\n",
      "+ echo 'Performing final conversion to a custom data format'\n",
      "+ python parquet_to_binary.py --parallel_jobs 40 --src_dir /data/dlrm/spark/output --intermediate_dir /data/dlrm/intermediate_binary --dst_dir /data/dlrm/binary_dataset\n",
      "\r",
      "0it [00:00, ?it/s]\n",
      "\r",
      "0it [00:00, ?it/s]\n",
      "\r",
      "0it [00:00, ?it/s]cat: '/data/dlrm/intermediate_binary/train/*.bin': No such file or directory\n",
      "cat: '/data/dlrm/intermediate_binary/test/*.bin': No such file or directory\n",
      "cat: '/data/dlrm/intermediate_binary/valid/*.bin': No such file or directory\n",
      "\n",
      "+ cp /data/dlrm/spark/output/model_size.json /data/dlrm/binary_dataset/model_size.json\n",
      "cp: cannot stat '/data/dlrm/spark/output/model_size.json': No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'apt install psmisc\\ncd ./preproc\\nkillall -9 java\\nbash ./prepare_dataset.sh\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-18be10c9d79c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apt install psmisc\\ncd ./preproc\\nkillall -9 java\\nbash ./prepare_dataset.sh\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'apt install psmisc\\ncd ./preproc\\nkillall -9 java\\nbash ./prepare_dataset.sh\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt install psmisc\n",
    "cd ./preproc\n",
    "killall -9 java\n",
    "bash ./prepare_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /data/dlrm/binary_dataset/model_size.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /workspace/dlrm/dlrm/scripts/main.py \\\n",
    "--mode train \\\n",
    "--dataset /data/dlrm/binary_dataset/ \\\n",
    "--num_numerical_features 1 \\\n",
    "--base_device cuda \\\n",
    "--lr 0.1 \\\n",
    "--embedding_type joint \\\n",
    "--epochs 10 \\\n",
    "--batch_size=8192 \\\n",
    "--save_checkpoint_path ./movie_lens_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python  /workspace/dlrm/dlrm/scripts/main.py \\\n",
    "--mode train \\\n",
    "--dataset /data/dlrm/binary_dataset/ \\\n",
    "--num_numerical_features 1 \\\n",
    "--base_device cuda \\\n",
    "--lr 0.1 \\\n",
    "--embedding_type joint \\\n",
    "--epochs 10 \\\n",
    "--batch_size=8192 \\\n",
    "--save_checkpoint_path ./movie_lens_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
