#!/bin/bash
#SBATCH -p batch               # partition
#SBATCH -N 1                   # number of nodes
#SBATCH -t 1:30:00              # wall time
#SBATCH -J "bert_pyt_lamb"                   # job name
#SBATCH --exclusive             # exclusive node access
#SBATCH --mem=0                 # all mem avail
#SBATCH --ntasks-per-node=16     # max 8 tasks per machine (one task per gpu) - Exception for pytorch// srun launch with -n1
#SBATCH --threads-per-core=2    # HT is on
#SBATCH --cpus-per-task=40      # Not used yet (to reach perf pytorch might need overcommit)
#SBATCH --overcommit            # Needed for pytorch
#SBATCH --mail-user=sharatht@nvidia.com
#SBATCH --mail-type=END
##SBATCH --deadline=$(date -d '+72 hours' '+%FT%T')

##SBATCH --reservation mlperf   # reservation name
##SBATCH --output=./logs/pytorch_%j.out
##SBATCH --exclude=sc-sdgx-[394,397]     # targeting nodes with mask until the constraints are implemented
##SBATCH -w sc-sdgx-[377-388],sc-sdgx-[394-408] # avail pod12
##SBATCH -C pod14               # constraint (not implemented yet)
##SBATCH --ntasks-per-socket=4  # Not used (our slurm does not have sockets defined)


## Your data, your container and its volumes
## Your data, your container and its volumes
set -x
DATESTAMP=${DATESTAMP:-`date +'%y-%m-%d-%H-%M-%S-%N'`}
BENCHMARK=${BENCHMARK:-"bert"}
FRAMEWORK=${FRAMEWORK:-"pytorch"}
BENCHMARK_NAME==${FRAMEWORK:-"bert"}
JOBNAME=${JOBNAME:-"bert_lamb_phase1_96n_wiki+books_only_fast_lamb_O1_run_1337"}
#.$DATESTAMP
# Create results directory

#DATADIR=${DATADIR:-"/raid/datasets/bert/hdf5/shard_1472_test_split_10/seq_128_pred_20_dupe_5/training"}
#DATADIR_PHASE2=${DATADIR_PHASE2:-"/raid/datasets/bert/hdf5/shard_1472_test_split_10/seq_512_pred_80_dupe_5/training"}
DATADIR="/raid/datasets/bert/hdf5/shard_1472_test_split_10/seq_128_pred_20_dupe_5/training"
DATADIR_PHASE2="/raid/datasets/bert/hdf5/shard_1472_test_split_10/seq_512_pred_80_dupe_5/training"
#BOOKS_DIR=/raid/datasets/seq_512_pred_80_dupe_5_shard_256
VOCAB_PATH=${VOCAB_PATH:-"/raid/datasets/bert_vocab/vocab.txt"}
DATASET=${DATASET:-"coco/coco-2014"}
CODEDIR=${CODEDIR:="bert_pyt/tree/sharatht/fast_lamb_ci_runs"}
CONT=${CONT:-"gitlab-master.nvidia.com/dl/JoC/bert_pyt:bert_pyt"}
LOGDIR=${LOGDIR:-"/raid/results/$BENCHMARK"}
NEXP=${NEXP:-1}
SEED=${SEED:-$(od -A n -t d -N 3 /dev/urandom)}
#CHECKPOINT_DIR=${CHECKPOINT_DIR:-"/gpfs/fs1/svcnvdlfw/7108495/results/output"}
CHECKPOINT_DIR="/gpfs/fs1/svcnvdlfw/7588296/results/output"
## Load system-specific parameters for benchmark
DGXSYSTEM=${DGXSYSTEM:-"DGX1"}
if [[ ! -f "config_${DGXSYSTEM}.sh" ]]; then
  echo "Unknown system, assuming DGX1"
  DGXSYSTEM="DGX1"
fi
source config_${DGXSYSTEM}.sh

IBDEVICES=${IBDEVICES:-$DGXIBDEVICES}

## Check whether we are running in a slurm env
INSLURM=1
if [[ -z "$SLURM_JOB_ID" ]]; then
  INSLURM=0
  export SLURM_JOB_ID="${DATESTAMP}"
  export SLURM_NNODES=1
else
  env | grep SLURM
fi
if [[ -z "SLURM_JOB_ID" || $SLURM_NNODES -eq 1 ]]; then
  # don't need IB if not multi-node
  export IBDEVICES=""
fi

# Create results directory
LOGFILE_BASE="${LOGDIR}/${DATESTAMP}"
mkdir -p $(dirname "${LOGFILE_BASE}")

## Docker params
CONTVOLS="-v $DATADIR:/workspace/data -v $LOGDIR:/results -v $CHECKPOINT_DIR:/checkpoints -v $DATADIR_PHASE2:/workspace/data_phase2"
NV_GPU="${NVIDIA_VISIBLE_DEVICES:-$(seq 0 $((${SLURM_NTASKS_PER_NODE:-${DGXNGPU}}-1)) | tr '\n' ',' | sed 's/,$//')}"
DOCKEREXEC="env NV_GPU=${NV_GPU} nvidia-docker run --init --rm --net=host --uts=host --ipc=host --ulimit stack=67108864 --ulimit memlock=-1 --name=cont_${SLURM_JOB_ID} --security-opt seccomp=unconfined $IBDEVICES"

## Get version of the OS
export MLPERF_HOST_OS="$(cat /etc/issue | head -1 | cut -f1-3 -d" ") / $(cat /etc/dgx-release | grep -E "DGX_PRETTY_NAME|DGX_OTA_VERSION" |cut -f2 -d= |cut -f2 -d '"' |paste -sd' ')"

## Prep run and launch
MASTER_IP=`getent hosts \`hostname\` | cut -d ' ' -f1`
PORT=$((4242 + RANDOM%1000))
SSH=''
SRUN=''
if [[ $INSLURM -eq 1 ]]; then
  hosts=( `scontrol show hostname |tr "\n" " "` )
  SSH='ssh -q -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no $hostn'
  SRUN='srun --mem=0 -N 1 -n 1 -w $hostn'
else
  hosts=( `hostname` )
fi

# Pull latest image
if [[ "${PULL}" != "0" ]]; then
  DOCKERPULL="docker pull $CONT"
  pids=();
  for hostn in ${hosts[@]}; do
    timeout -k 600s 600s \
      $(eval echo $SRUN) $DOCKERPULL &
    pids+=($!);
  done
  wait "${pids[@]}"
  success=$? ; if [ $success -ne 0 ]; then echo "ERR: Image pull failed."; exit $success ; fi
fi

# Test the base container launch
pids=();
for hostn in ${hosts[@]}; do
  timeout -k 600s 600s \
    $(eval echo $SRUN) $DOCKEREXEC $CONT python -c 'import torch; print("Found",torch.cuda.device_count(),"CUDA GPUs")' &
  pids+=($!);
done
wait "${pids[@]}"
success=$? ; if [ $success -ne 0 ]; then echo "ERR: Base container launch failed."; exit $success ; fi

# Launch containers
pids=(); rets=()
for hostn in ${hosts[@]}; do
  $(eval echo $SSH) $DOCKEREXEC $CONTVOLS $CONT sleep infinity &
  pids+=($!); rets+=($?);
done
success=0; for s in ${rets[@]}; do ((success+=s)); done ; if [ $success -ne 0 ]; then echo "ERR: Container launch failed."; exit $success ; fi
sleep 30 # Making sure containers have time to launch 

# Disable compat check from further running
pids=(); rets=()
for hostn in ${hosts[@]}; do
  $(eval echo $SSH) docker exec cont_${SLURM_JOB_ID} rm -f /etc/shinit &
  pids+=($!);
done
wait "${pids[@]}"

# Run benchmarks

export SEED
export NEXP
for nrun in `seq 1 $NEXP`; do
  (
    echo "Beginning trial $nrun of $NEXP"

    export VARS=(
        "-e" "SLURM_NNODES=$SLURM_NNODES"
        "-e" "MLPERF_HOST_OS"
    )


	## Clear RAM cache dentries and inodes
    echo "Clearing caches"
    pids=(); rets=()
    for hostn in ${hosts[@]}; do
      if [[ $INSLURM -eq 1 ]]; then
        $(eval echo $SSH) bash -c 'sync && sudo /sbin/sysctl vm.drop_caches=3' &
      else
        docker run --init --rm --privileged --entrypoint bash $CONT -c "sync && echo 3 > /proc/sys/vm/drop_caches || exit 1" &
      fi
      pids+=($!); rets+=($?);
    done
    wait "${pids[@]}"
    success=0; for s in ${rets[@]}; do ((success+=s)); done ; if [ $success -ne 0 ]; then echo "ERR: Cache clearing failed."; exit $success ; fi

	## Launching benchmark 
    pids=();
    export MULTI_NODE=''
    for h in `seq 0 $((SLURM_NNODES-1))`; do
      hostn="${hosts[$h]}"
      echo "Launching on node $hostn"
      if [[ $SLURM_NNODES -gt 1 ]]; then
        export MULTI_NODE=" --nnodes=$SLURM_NNODES --node_rank=$h --master_addr=$MASTER_IP --master_port=$PORT"
      else
        export MULTI_NODE=" --master_port=$PORT"
      fi
      export DOCKERENV=(
         "-e" "DGXSYSTEM=$DGXSYSTEM"
         "-e" "MULTI_NODE=$MULTI_NODE"
         "-e" "SEED=$SEED"
         "-e" "SLURM_JOB_ID=$SLURM_JOB_ID"
         "-e" "SLURM_NTASKS_PER_NODE=$SLURM_NTASKS_PER_NODE"
         "-e" "SLURM_NNODES=$SLURM_NNODES"
      )
      # Execute command
      set -x
      $(eval echo $SRUN) docker exec "${DOCKERENV[@]}" -e MODE=TRAIN cont_${SLURM_JOB_ID} ./run_and_time.sh &
      pids+=($!);
      set +x
    done
    wait "${pids[@]}"

  ) |& tee ${LOGFILE_BASE}_$nrun.log

  ## SEED update
  export SEED=$(od -A n -t d -N 3 /dev/urandom)

done

# Clean up (note: on SLURM we skip this, as the epilogue will take care of it)
if [[ $INSLURM -eq 0 ]]; then
  docker rm -f cont_${SLURM_JOB_ID}
fi
