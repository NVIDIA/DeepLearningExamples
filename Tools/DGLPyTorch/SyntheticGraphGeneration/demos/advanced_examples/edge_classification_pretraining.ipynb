{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ea7ceaf7",
   "metadata": {},
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a2027",
   "metadata": {},
   "source": [
    "# Edge Classification Pretraining demo (IEEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3d798",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Often times it is helpful to pre-train or initialize a network with learned weights on a downstream task of interest and further fine-tune.\n",
    "\n",
    "This notebook demonstrates the steps for pretraing a GNN on synthetic data and finetuning on real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f39e76",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f315cfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "from syngen.preprocessing.datasets.ieee import IEEEPreprocessing\n",
    "\n",
    "# generation\n",
    "from syngen.synthesizer import StaticBipartiteGraphSynthesizer\n",
    "from syngen.generator.tabular import CTGANGenerator\n",
    "from syngen.generator.graph import RMATBipartiteGenerator\n",
    "from syngen.graph_aligner.xgboost_aligner import XGBoostAligner\n",
    "\n",
    "# training\n",
    "import torch\n",
    "from syngen.benchmark.data_loader.datasets.edge_ds import EdgeDS\n",
    "from syngen.benchmark.models import GATEC\n",
    "from syngen.benchmark.tasks.ec import train_epoch\n",
    "\n",
    "# utils\n",
    "import dgl\n",
    "import time\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from syngen.utils.types import MetaData, ColumnType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3e3a6",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3db76c",
   "metadata": {},
   "source": [
    "In the following cells, a synthesizer is instantiated and fitted on the IEEE dataset.\n",
    "\n",
    "Once fitted, the synthesizer is used to generate synthetic data with similar characteristics.\n",
    "\n",
    "For a more detailed explanation checkout the `e2e_ieee_demo.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f86bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Initialized logger\n",
      "DEBUG:root:Using seed: 42\n"
     ]
    }
   ],
   "source": [
    "edge_feature_generator = CTGANGenerator(epochs=10, batch_size=2000)\n",
    "static_graph_generator = RMATBipartiteGenerator(seed=42)\n",
    "preprocessing = IEEEPreprocessing(cached=False)\n",
    "graph_aligner = XGBoostAligner(features_to_correlate_edge={'TransactionAmt': ColumnType.CONTINUOUS})\n",
    "\n",
    "num_edges = 52008\n",
    "num_nodes_src_set = 17091\n",
    "num_nodes_dst_set = 198\n",
    "num_edges_src_dst = num_edges\n",
    "num_edges_dst_src = num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bb8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = StaticBipartiteGraphSynthesizer(\n",
    "                                    graph_generator=static_graph_generator,\n",
    "                                    graph_info=preprocessing.graph_info,\n",
    "                                    edge_feature_generator=edge_feature_generator,\n",
    "                                    graph_aligner=graph_aligner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d4eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:syngen.preprocessing.base_preprocessing:read data : (52008, 386)\n",
      "INFO:syngen.preprocessing.base_preprocessing:droping column: []\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing.transform('/workspace/data/ieee-fraud/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873d0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Fitting feature generator...\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Fitting graph generator...\n",
      "DEBUG:root:Fit results dst_src: None\n",
      "DEBUG:root:Fit results src_dst: (0.43224427700042733, 0.22040712833404563, 0.06775572299957267, 0.27959287166595437)\n",
      "Fitting TransactionAmt ...\n",
      "[08:01:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.08987\n",
      "[1]\ttrain-rmse:1.07147\n",
      "[2]\ttrain-rmse:1.05633\n",
      "[3]\ttrain-rmse:1.03909\n",
      "[4]\ttrain-rmse:1.02918\n",
      "[5]\ttrain-rmse:1.02108\n",
      "[6]\ttrain-rmse:1.01415\n",
      "[7]\ttrain-rmse:1.00876\n",
      "[8]\ttrain-rmse:1.00009\n",
      "[9]\ttrain-rmse:0.99292\n"
     ]
    }
   ],
   "source": [
    "synthesizer.fit(edge_data = data[MetaData.EDGE_DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f1603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Generating graph...\n",
      "INFO:syngen.utils.gen_utils:writing to file table_edge_samples_52018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Generating final graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligner - preds edge: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 74.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ranking, overlaying features on generated graph...\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Saving data to ./\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = synthesizer.generate(num_nodes_src_set, num_nodes_dst_set, num_edges_src_dst, num_edges_dst_src, graph_noise=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e21408",
   "metadata": {},
   "source": [
    "### Train GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834318e",
   "metadata": {},
   "source": [
    "To train an example GNN we need the following:\n",
    "\n",
    "- a dataset object instantiated using either the synthetic or original data\n",
    "- the model, optimizer and hyperparameters defined\n",
    "\n",
    "In the tool an example dataloader is implemented for edge classification under `syngen/benchmark/data_loader`.\n",
    "\n",
    "This dataset object is used to great the dgl graphs corresponding to both the generated data and real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fabfa9",
   "metadata": {},
   "source": [
    "#### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e8bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EdgeDS(target_col='isFraud')\n",
    "g, e_ids = dataset.get_graph(data[MetaData.EDGE_DATA],graph_info=preprocessing.graph_info)\n",
    "gs, es_ids = dataset.get_graph(synthetic_data[MetaData.EDGE_DATA],graph_info=preprocessing.graph_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830709c",
   "metadata": {},
   "source": [
    "#### Create helper function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959a3a2",
   "metadata": {},
   "source": [
    "The helper function defines a simple trianing loop and standard metrics for edge classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4bec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, graph, edge_ids, epochs, shuffle=True, batch_size=256):\n",
    "    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "    dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "        graph, edge_ids, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=False,\n",
    "        num_workers=8)\n",
    "    \n",
    "    best_val_acc, best_test_acc = 0, 0\n",
    "    total_batches = []\n",
    "    batch_times = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        train_acc, val_acc, test_acc, losses, e_batch_times = train_epoch(model, dataloader, optimizer, verbose=True)\n",
    "        if e == 0:\n",
    "            e_batch_times = e_batch_times[10:]  # ignore warm-up steps\n",
    "        batch_times += e_batch_times\n",
    "        \n",
    "        val_acc = np.mean(val_acc)\n",
    "        test_acc = np.mean(test_acc)\n",
    "        train_acc = np.mean(train_acc)\n",
    "        loss = np.mean(losses)\n",
    "\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        if (e+1) % 1 == 0:\n",
    "            print('epoch {}, loss: {:.3f}, train acc: {:.3f} val acc: {:.4f} (best {:.4f}), test acc: {:.4f} (best {:.4f})'.format(\n",
    "                e+1, loss, train_acc, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "    return best_val_acc.item(), best_test_acc.item(), np.mean(batch_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cea06",
   "metadata": {},
   "source": [
    "#### No-Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093203f8",
   "metadata": {},
   "source": [
    "Without pre-training the model is trained from scratch using the original data graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ab387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.123, train acc: 0.982 val acc: 0.9823 (best 0.9823), test acc: 0.9836 (best 0.9836)\n",
      "epoch 2, loss: 0.087, train acc: 0.983 val acc: 0.9837 (best 0.9837), test acc: 0.9835 (best 0.9835)\n",
      "epoch 3, loss: 0.087, train acc: 0.983 val acc: 0.9836 (best 0.9837), test acc: 0.9838 (best 0.9835)\n",
      "epoch 4, loss: 0.087, train acc: 0.983 val acc: 0.9829 (best 0.9837), test acc: 0.9835 (best 0.9835)\n",
      "epoch 5, loss: 0.087, train acc: 0.983 val acc: 0.9831 (best 0.9837), test acc: 0.9838 (best 0.9835)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9836600881581213, 0.9835433869385252, 0.07241769592729536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_feats = g.ndata.get('feat').shape[1]\n",
    "in_edge_feats = g.edata.get('feat').shape[1]\n",
    "model = GATEC(\n",
    "    in_dim=in_feats, \n",
    "    in_dim_edge=in_edge_feats, \n",
    "    hidden_dim=64, \n",
    "    out_dim=32, \n",
    "    num_classes=2, \n",
    "    n_heads=2,\n",
    "    in_feat_dropout=0.2,\n",
    "    dropout=0.2,\n",
    "    n_layers=1).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0)\n",
    "train(model, optimizer, g, e_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5280a",
   "metadata": {},
   "source": [
    "#### Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bebba4",
   "metadata": {},
   "source": [
    "In this example the model is first trained on the generated data for a certain epoch budget.\n",
    "\n",
    "Subsequently it is further trained on the original data graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21ab679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining\n",
      "epoch 1, loss: 0.430, train acc: 0.855 val acc: 0.8576 (best 0.8576), test acc: 0.8483 (best 0.8483)\n",
      "epoch 2, loss: 0.387, train acc: 0.855 val acc: 0.8551 (best 0.8576), test acc: 0.8482 (best 0.8483)\n",
      "epoch 3, loss: 0.344, train acc: 0.855 val acc: 0.8570 (best 0.8576), test acc: 0.8473 (best 0.8483)\n",
      "epoch 4, loss: 0.340, train acc: 0.854 val acc: 0.8581 (best 0.8581), test acc: 0.8499 (best 0.8499)\n",
      "epoch 5, loss: 0.340, train acc: 0.854 val acc: 0.8557 (best 0.8581), test acc: 0.8500 (best 0.8499)\n",
      "Fine-tuning\n",
      "epoch 1, loss: 0.098, train acc: 0.983 val acc: 0.9836 (best 0.9836), test acc: 0.9838 (best 0.9838)\n",
      "epoch 2, loss: 0.087, train acc: 0.983 val acc: 0.9836 (best 0.9836), test acc: 0.9841 (best 0.9841)\n",
      "epoch 3, loss: 0.087, train acc: 0.983 val acc: 0.9830 (best 0.9836), test acc: 0.9833 (best 0.9841)\n",
      "epoch 4, loss: 0.087, train acc: 0.983 val acc: 0.9832 (best 0.9836), test acc: 0.9834 (best 0.9841)\n",
      "epoch 5, loss: 0.087, train acc: 0.983 val acc: 0.9831 (best 0.9836), test acc: 0.9839 (best 0.9841)\n"
     ]
    }
   ],
   "source": [
    "model_pretrain = GATEC(\n",
    "    in_dim=in_feats, \n",
    "    in_dim_edge=in_edge_feats, \n",
    "    hidden_dim=64, \n",
    "    out_dim=32, \n",
    "    num_classes=2, \n",
    "    n_heads=2,\n",
    "    in_feat_dropout=0.2,\n",
    "    dropout=0.2,\n",
    "    n_layers=1).cuda()\n",
    "\n",
    "print('Pretraining')\n",
    "optimizer_pretrain = torch.optim.Adam(model_pretrain.parameters(),lr=0.0005,weight_decay=0.0)\n",
    "_, _, synthetic_graph_throughput_batches = train(model_pretrain, optimizer_pretrain, gs, es_ids, 5)\n",
    "\n",
    "print('Fine-tuning')\n",
    "optimizer_pretrain = torch.optim.Adam(model_pretrain.parameters(),lr=0.0005,weight_decay=0.0)\n",
    "val_acc, test_acc, original_graph_throughput_batches = train(model_pretrain, optimizer_pretrain, g, e_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b615c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07172738509898138, 0.07249173903376749)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_graph_throughput_batches, original_graph_throughput_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9e95c",
   "metadata": {},
   "source": [
    "### CLI example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd05a0",
   "metadata": {},
   "source": [
    "The tool also provides this functionality through its CLI.\n",
    "\n",
    "The commands used to generate and pretrain/fine tune on the downstream tasks as done above are provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de441fe",
   "metadata": {},
   "source": [
    "#### Generate synthetic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af89d214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=========================================\n",
      "INFO:__main__:|    Synthetic Graph Generation Tool    |\n",
      "INFO:__main__:=========================================\n",
      "INFO:syngen.preprocessing.base_preprocessing:read data : (52008, 386)\n",
      "INFO:syngen.preprocessing.base_preprocessing:droping column: []\n",
      "DEBUG:root:Initialized logger\n",
      "DEBUG:root:Using seed: 42\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Fitting feature generator...\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Fitting graph generator...\n",
      "DEBUG:root:Fit results dst_src: None\n",
      "DEBUG:root:Fit results src_dst: (0.43224427700042733, 0.22040712833404563, 0.06775572299957267, 0.27959287166595437)\n",
      "DEBUG:asyncio:Using selector: EpollSelector\n",
      "DEBUG:asyncio:Using selector: EpollSelector\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "Fitting TransactionAmt ...\n",
      "[08:44:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:219.15953\n",
      "[1]\ttrain-rmse:213.89105\n",
      "[2]\ttrain-rmse:209.52629\n",
      "[3]\ttrain-rmse:205.01221\n",
      "[4]\ttrain-rmse:202.09795\n",
      "[5]\ttrain-rmse:199.70622\n",
      "[6]\ttrain-rmse:197.68074\n",
      "[7]\ttrain-rmse:196.07938\n",
      "[8]\ttrain-rmse:193.95419\n",
      "[9]\ttrain-rmse:192.18870\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Generating graph...\n",
      "INFO:syngen.utils.gen_utils:writing to file /workspace/ieee/table_edge_samples_52018.csv\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Generating final graph...\n",
      "Aligner - preds edge: 100%|█████████████████████| 52/52 [00:00<00:00, 80.25it/s]\n",
      "Finished ranking, overlaying features on generated graph...\n",
      "INFO:syngen.synthesizer.static_bipartite_graph_synthesizer:Saving data to /workspace/ieee/\n",
      "INFO:__main__:Done synthesizing dataset...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m syngen synthesize \\\n",
    "--synthesizer static_bipartite_graph \\\n",
    "--preprocessing ieee \\\n",
    "--aligner xg_boost \\\n",
    "--graph-generator rmat_bipartite \\\n",
    "--gg-seed 42 \\\n",
    "--edge-generator ctgan \\\n",
    "--eg-batch-size 2000 \\\n",
    "--eg-epochs 10 \\\n",
    "--num-nodes-src-set 17091 \\\n",
    "--num-nodes-dst-set 198 \\\n",
    "--num-edges-src-dst 52008 \\\n",
    "--num-edges-dst-src 52008 --data-path '/workspace/data/ieee-fraud/data.csv' --save-path '/workspace/ieee/' \\\n",
    "--features-to-correlate-edge \"{\\\"TransactionAmt\\\": \\\"continuous\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef4fb7",
   "metadata": {},
   "source": [
    "#### Results without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65ab4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=========================================\n",
      "INFO:__main__:|    Synthetic Graph Generation Tool    |\n",
      "INFO:__main__:=========================================\n",
      "INFO:syngen.preprocessing.base_preprocessing:read data : (52008, 386)\n",
      "INFO:syngen.preprocessing.base_preprocessing:droping column: []\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 0, loss: 0.142, val acc: 0.985 (best 0.985), test acc: 0.985 (best 0.985)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 1, loss: 0.089, val acc: 0.985 (best 0.985), test acc: 0.985 (best 0.985)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 2, loss: 0.089, val acc: 0.985 (best 0.985), test acc: 0.985 (best 0.985)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 3, loss: 0.089, val acc: 0.985 (best 0.985), test acc: 0.985 (best 0.985)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 4, loss: 0.089, val acc: 0.985 (best 0.985), test acc: 0.985 (best 0.985)\n",
      "INFO:__main__:{'finetune-loss': 0.08860890291558177, 'finetune-val-acc': 0.9854286459146762, 'finetune-test-acc': 0.9847563131182802}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m syngen pretrain \\\n",
    "--model gat_ec \\\n",
    "--hidden-dim 64 \\\n",
    "--out-dim 32 \\\n",
    "--n-layers 1 \\\n",
    "--n-heads 2 \\\n",
    "--weight-decay 0.0 \\\n",
    "--learning-rate 0.0005 \\\n",
    "--batch-size 256 \\\n",
    "--pretrain-epochs 0 \\\n",
    "--finetune-epochs 5 \\\n",
    "--data-path '/workspace/data/ieee-fraud/data.csv' \\\n",
    "--pretraining-path '/workspace/ieee/' \\\n",
    "--preprocessing ieee \\\n",
    "--task ec \\\n",
    "--target-col isFraud \\\n",
    "--num-classes 2 \\\n",
    "--log-interval 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6655f58",
   "metadata": {},
   "source": [
    "#### Pretrain and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=========================================\n",
      "INFO:__main__:|    Synthetic Graph Generation Tool    |\n",
      "INFO:__main__:=========================================\n",
      "INFO:syngen.preprocessing.base_preprocessing:read data : (52008, 386)\n",
      "INFO:syngen.preprocessing.base_preprocessing:droping column: []\n",
      "INFO:syngen.benchmark.tasks.ec:Running pretraining ...\n",
      "INFO:syngen.benchmark.tasks.ec:Pretraining epoch 0, loss: 0.046, val acc: 1.000 (best 1.000), test acc: 1.000 (best 1.000)\n",
      "INFO:syngen.benchmark.tasks.ec:Pretraining epoch 1, loss: 0.000, val acc: 1.000 (best 1.000), test acc: 1.000 (best 1.000)\n",
      "INFO:syngen.benchmark.tasks.ec:Pretraining epoch 2, loss: 0.000, val acc: 1.000 (best 1.000), test acc: 1.000 (best 1.000)\n",
      "INFO:syngen.benchmark.tasks.ec:Pretraining epoch 3, loss: 0.000, val acc: 1.000 (best 1.000), test acc: 1.000 (best 1.000)\n",
      "INFO:syngen.benchmark.tasks.ec:Pretraining epoch 4, loss: 0.000, val acc: 1.000 (best 1.000), test acc: 1.000 (best 1.000)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 0, loss: 0.097, val acc: 0.982 (best 0.982), test acc: 0.982 (best 0.982)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 1, loss: 0.087, val acc: 0.982 (best 0.982), test acc: 0.982 (best 0.982)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 2, loss: 0.086, val acc: 0.982 (best 0.982), test acc: 0.982 (best 0.982)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 3, loss: 0.086, val acc: 0.982 (best 0.982), test acc: 0.982 (best 0.982)\n",
      "INFO:syngen.benchmark.tasks.ec:Finetuning: In epoch 4, loss: 0.086, val acc: 0.982 (best 0.982), test acc: 0.982 (best 0.982)\n",
      "INFO:__main__:{'finetune-loss': 0.08606245489699729, 'finetune-val-acc': 0.9823067004189772, 'finetune-test-acc': 0.9821474289192873, 'pretrain-loss': 1.0629034280790097e-06, 'pretrain-val-acc': 1.0, 'pretrain-test-acc': 1.0}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m syngen pretrain \\\n",
    "--model gat_ec \\\n",
    "--hidden-dim 64 \\\n",
    "--out-dim 32 \\\n",
    "--n-layers 1 \\\n",
    "--n-heads 2 \\\n",
    "--weight-decay 0.0 \\\n",
    "--learning-rate 0.0005 \\\n",
    "--batch-size 256 \\\n",
    "--pretrain-epochs 5 \\\n",
    "--finetune-epochs 5 \\\n",
    "--data-path '/workspace/data/ieee-fraud/data.csv' \\\n",
    "--pretraining-path '/workspace/ieee/' \\\n",
    "--preprocessing ieee \\\n",
    "--task ec \\\n",
    "--target-col isFraud \\\n",
    "--num-classes 2 \\\n",
    "--log-interval 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da530b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
